{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH, MODELS_PATH, RAW_PATH\n",
    "import torch\n",
    "import os\n",
    "from datasets import Audio, load_dataset, Dataset\n",
    "from src.jamendo_utils import read_file\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import ClapConfig, ClapModel, AutoFeatureExtractor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import default_collate\n",
    "import tqdm\n",
    "import json\n",
    "from audiocraft.data.audio import audio_read, audio_write\n",
    "from audiocraft.data.audio_utils import convert_audio_channels, convert_audio\n",
    "\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "from audiocraft.models import MusicGen\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\").to(DEVICE)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"laion/clap-htsat-unfused\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ds = load_dataset('csv', data_files=[\n",
    "    RAW_PATH('song_describer', 'song_describer.csv'),\n",
    "], split='train')\n",
    "captions = {}\n",
    "for row in ds:\n",
    "    idx = row['track_id']\n",
    "    captions[idx] = captions.get(idx, [])\n",
    "    captions[idx].append(row['caption'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "base_dir = RAW_PATH('song_describer', 'audio')\n",
    "\n",
    "\n",
    "def map_path(ex):\n",
    "    ex['audio'] = os.path.join(base_dir, ex['path'].replace('.mp3', '.2min.mp3'))\n",
    "    return ex\n",
    "\n",
    "\n",
    "def show_audio(ds, sec=10):\n",
    "    for song in ds['audio']:\n",
    "        display_audio(torch.Tensor(np.array(song['array']))[:song['sampling_rate'] * sec][None], song['sampling_rate'])\n",
    "\n",
    "\n",
    "ds = load_dataset('csv', data_files=[\n",
    "    RAW_PATH('song_describer', 'song_describer.csv'),\n",
    "], split='train').map(map_path)\n",
    "ds = Dataset.from_pandas(ds.to_pandas().drop_duplicates(['track_id']))\n",
    "tracks, tags, extra = read_file(RAW_PATH('song_describer', 'song_describer_14_04_23.mtg-jamendo.tsv'))\n",
    "pop_artists = pd.Series([x['artist_id'] for k, x in tracks.items()]).value_counts().nlargest(5).index.tolist()\n",
    "pop_artists"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sr = 44100\n",
    "pop_artis = pop_artists[1]\n",
    "theme_filter = lambda x: x['track_id'] in tags['mood/theme']['relaxing']\n",
    "pop_filter = lambda x: x['artist_id'] == pop_artis\n",
    "ds_filtered = ds.cast_column('audio', Audio(sampling_rate=48000))\n",
    "show_audio(ds_filtered.take(3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "embeds = []\n",
    "ids = []\n",
    "with torch.no_grad():\n",
    "    for row in tqdm.tqdm(ds_filtered):\n",
    "        audio = row['audio']\n",
    "        inputs = feature_extractor(torch.tensor(audio['array']), return_tensors=\"pt\",\n",
    "                                   sampling_rate=audio['sampling_rate'])\n",
    "        ids.append(row['track_id'])\n",
    "        embeds.append(model.get_audio_features(**inputs, ))\n",
    "\n",
    "dim_input = torch.stack(embeds).squeeze().cpu().numpy()\n",
    "reducer = umap.UMAP(n_neighbors=5, n_components=2, metric='cosine')\n",
    "# reducer = PCA(n_components=2)\n",
    "\n",
    "embeddings_2d = reducer.fit_transform(dim_input)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n_clusters = 12\n",
    "clst = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "# clst = DBSCAN(eps=0.06, min_samples=5)\n",
    "\n",
    "labels = clst.fit_predict(embeddings_2d)\n",
    "clusters = {}\n",
    "rev_lab = {}\n",
    "for idx, label in enumerate(labels):\n",
    "    rev_lab[ids[idx]] = idx\n",
    "    if label not in clusters:\n",
    "        clusters[label] = set()\n",
    "    clusters[label].add(ids[idx])\n",
    "ds_filtered = ds_filtered.map(lambda x, idx: {'clst': labels[idx]}, with_indices=True)\n",
    "\n",
    "for i, idxs in clusters.items():\n",
    "    a_id = [rev_lab[x] for x in idxs]\n",
    "    plt.scatter(embeddings_2d[a_id, 0], embeddings_2d[a_id, 1], label=f'cluster {i}')\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cl_ids = set(clusters[10])\n",
    "id_filter = lambda x: x['track_id'] in cl_ids\n",
    "\n",
    "show_audio(ds_filtered.filter(id_filter, num_proc=12).take(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ds_filtered.remove_columns(['audio']).to_json(RAW_PATH('song_describer', 'clap_clustered.json'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.read_json(RAW_PATH('song_describer', 'clap_clustered.jsonl'), lines=True)[['track_id', 'clst']]\n",
    "df['genre'] = df.apply(lambda x: tracks.get(x['track_id'], {}).get('genre', set()), axis=1)\n",
    "df['instrument'] = df.apply(lambda x: tracks.get(x['track_id'], {}).get('instrument', set()), axis=1)\n",
    "df['mood/theme'] = df.apply(lambda x: tracks.get(x['track_id'], {}).get('mood/theme', set()), axis=1)\n",
    "df['path'] = df.apply(lambda x: tracks.get(x['track_id'], {}).get('path', \"\").replace('.mp3', '.2min.mp3'), axis=1)\n",
    "df['descriptions'] = df.apply(lambda x: captions.get(x['track_id'], []), axis=1)\n",
    "df['text_clst'] = df.apply(lambda x: text_clustered.get(x['track_id'], []), axis=1)\n",
    "df.to_json(RAW_PATH('ds-indexes', 'song-describer.jsonl'), lines=True, orient=\"records\", force_ascii=False)\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "grouped_counts = {}\n",
    "clusters = {}\n",
    "\n",
    "for cluster, group in df.groupby('clst'):\n",
    "    def col(col_name):\n",
    "        flattened_values = [item for subset in group[col_name] for item in subset]\n",
    "        value_counts = Counter(flattened_values)\n",
    "        return dict(sorted(value_counts.items(), key=lambda item: item[1], reverse=True)[:5])\n",
    "\n",
    "\n",
    "    grouped_counts[cluster] = {\n",
    "        'genre': col('genre'),\n",
    "        'instrument': col('instrument'),\n",
    "        'mood/theme': col('mood/theme')\n",
    "    }\n",
    "    clusters[cluster] = group['track_id'].values.tolist()\n",
    "with open(RAW_PATH('song_describer', 'clusters_stats.json'), 'w') as fh:\n",
    "    json.dump(grouped_counts, fh, indent=4)\n",
    "with open(RAW_PATH('song_describer', 'clusters.json'), 'w') as fh:\n",
    "    json.dump(clusters, fh, indent=4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=5\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "def clear_if_exists(dir_name):\n",
    "    if os.path.exists(dir_name):\n",
    "        shutil.rmtree(dir_name)\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "for cluster, idxs in clusters.items():\n",
    "    rnd_idxs = random.sample(idxs, 20)\n",
    "    label = f'cluster_{cluster}'\n",
    "\n",
    "\n",
    "    def copy_files(split, idxs):\n",
    "        clear_if_exists(INPUT_PATH('textual-inversion-v3', 'data', split, label, 'audio'))\n",
    "        clear_if_exists(INPUT_PATH('textual-inversion-v3', 'data', split, label, 'encoded'))\n",
    "        res = []\n",
    "        for idx in tqdm.tqdm(idxs):\n",
    "            src_path = RAW_PATH('song_describer', 'audio', tracks.get(idx)['path'].replace('.mp3', '.2min.mp3'))\n",
    "            dest_path = tracks.get(idx)['path'].replace('.mp3', '.2min.mp3')\n",
    "            relative_path = os.path.join('data', split, label, 'audio', os.path.basename(dest_path))\n",
    "            enc_path = os.path.join('data', split, label, 'encoded',\n",
    "                                    os.path.basename(dest_path).replace('.2min.mp3', '.pt'))\n",
    "            dest_path = INPUT_PATH('textual-inversion-v3', relative_path)\n",
    "            shutil.copy2(src_path, dest_path)\n",
    "            with torch.no_grad():\n",
    "                music, sr = audio_read(dest_path)\n",
    "                music = music[None]\n",
    "                music = convert_audio(music, sr, 32000, 1)\n",
    "                encoded_music, _ = model.compression_model.encode(music.to(DEVICE))\n",
    "                torch.save(encoded_music.cpu(), INPUT_PATH('textual-inversion-v3', enc_path))\n",
    "            res.append({\n",
    "                'track_id': idx,\n",
    "                'audio_path': relative_path,\n",
    "                'encoded_path': enc_path,\n",
    "                'concept': label\n",
    "            })\n",
    "        return res\n",
    "\n",
    "\n",
    "    train_data.extend(copy_files('train', rnd_idxs[:10]))\n",
    "    val_data.extend(copy_files('valid', rnd_idxs[10:]))\n",
    "train_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open(INPUT_PATH('textual-inversion-v3', 'metadata_train.json'), 'w') as fh:\n",
    "    json.dump(train_data, fh, indent=4)\n",
    "with open(INPUT_PATH('textual-inversion-v3', 'metadata_val.json'), 'w') as fh:\n",
    "    json.dump(val_data, fh, indent=4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gpt_client = OpenAI()\n",
    "descriptions = df[['track_id', 'descriptions']].explode('descriptions', ignore_index=True)['descriptions'].tolist()\n",
    "embedings = gpt_client.embeddings.create(input=descriptions, model=\"text-embedding-3-small\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "client = chromadb.Client()\n",
    "client.delete_collection('audio_descriptions')\n",
    "collection = client.get_or_create_collection(\n",
    "    \"audio_descriptions\",\n",
    ")\n",
    "embeds = []\n",
    "for i, (_, row) in enumerate(df[['track_id', 'descriptions']].explode('descriptions', ignore_index=True).iterrows()):\n",
    "    track_id = row[\"track_id\"]\n",
    "    embeds.append(embedings.data[i].embedding)\n",
    "    collection.add(\n",
    "        documents=[row[\"descriptions\"]],\n",
    "        embeddings=[embedings.data[i].embedding],\n",
    "        metadatas=[{\"track_id\": track_id}],\n",
    "        ids=[str(uuid.uuid4())]\n",
    "    )\n",
    "dim_input = np.array(embeds)\n",
    "reducer = umap.UMAP(n_neighbors=5, n_components=2, metric='cosine')\n",
    "# reducer = PCA(n_components=2)\n",
    "\n",
    "embeddings_2d = reducer.fit_transform(dim_input)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "exploded_df = df[['track_id', 'descriptions']].explode('descriptions', ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# n_clusters = 12\n",
    "clst = KMeans()\n",
    "# clst = DBSCAN(eps=0.06, min_samples=5)\n",
    "\n",
    "labels = clst.fit_predict(embeddings_2d)\n",
    "exploded_df['clst'] = labels\n",
    "clusters = {}\n",
    "rev_lab = {}\n",
    "for idx, label in enumerate(labels):\n",
    "    if label not in clusters:\n",
    "        clusters[label] = []\n",
    "    clusters[label].append(idx)\n",
    "\n",
    "for i, idxs in clusters.items():\n",
    "    plt.scatter(embeddings_2d[idxs, 0], embeddings_2d[idxs, 1], label=f'cluster {i}')\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "exploded_df.groupby(\"track_id\", as_index=False).agg({\"clst\": list}).set_index('track_id')['clst'].to_json(\n",
    "    RAW_PATH('song_describer', 'openai_clustered.jsonl'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "exploded_df[exploded_df['clst'] == 4]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "query_text = \"relaxing piano music\"\n",
    "query_embedding = gpt_client.embeddings.create(input=[description], model=\"text-embedding-3-small\").data[0].embedding\n",
    "res = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3\n",
    ")\n",
    "res"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "res"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Audiocraft",
   "language": "python",
   "name": "audiocraft_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
