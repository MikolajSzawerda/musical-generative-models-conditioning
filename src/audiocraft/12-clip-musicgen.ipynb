{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T18:39:13.225900Z",
     "start_time": "2024-12-29T18:39:07.819246Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torchvision.datasets\n",
    "\n",
    "sys.path.append(os.path.abspath(\"src\"))\n",
    "\n",
    "from audiocraft.models import MusicGen\n",
    "import torch\n",
    "from tools.project import INPUT_PATH, MODELS_PATH\n",
    "from src.data import TextConcepts, TokensProvider\n",
    "from src.losses import compute_cross_entropy\n",
    "import tqdm\n",
    "import pytorch_lightning as L\n",
    "from src.model import TIMusicGen, ModelConfig\n",
    "from torch.optim import Adam\n",
    "from src.clip_textual_inversion import ConceptDataModule, ClipProjector\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src.img_feature_extractor import LitMNISTModel\n",
    "from torchvision import transforms\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TOKENS_NUM = 5\n",
    "NUM_WORKERS = int(os.cpu_count() * 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T18:39:25.588762Z",
     "start_time": "2024-12-29T18:39:19.174385Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszawerda/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "music_model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "music_model.set_generation_params(\n",
    "\tuse_sampling=True,\n",
    "\ttop_k=250,\n",
    "\tduration=5\n",
    ")\n",
    "text_conditioner = list(music_model.lm.condition_provider.conditioners.values())[0]\n",
    "tokenizer = text_conditioner.t5_tokenizer\n",
    "text_model = text_conditioner.t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T07:20:36.115700Z",
     "start_time": "2024-12-29T07:20:36.112449Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T07:20:36.147273Z",
     "start_time": "2024-12-29T07:20:36.139025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[8774,  296]]), 'attention_mask': tensor([[1, 1]])}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_prompt = tokenizer(\n",
    "\t['Hello world'], return_tensors=\"pt\", padding=True, add_special_tokens=False\n",
    ")\n",
    "tokenized_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T07:20:36.270945Z",
     "start_time": "2024-12-29T07:20:36.164052Z"
    }
   },
   "outputs": [],
   "source": [
    "custom = torch.rand_like(text_model.shared.weight[tokenized_prompt['input_ids']], requires_grad=True)\n",
    "embeds = text_model(inputs_embeds=custom, attention_mask=tokenized_prompt['attention_mask']).last_hidden_state\n",
    "loss = torch.norm(embeds)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T07:20:36.289502Z",
     "start_time": "2024-12-29T07:20:36.285543Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0575, -0.0300, -0.2495,  ..., -0.1649,  0.0237, -0.0073],\n",
       "         [ 0.1011,  0.0372, -0.0026,  ...,  0.1519, -0.0089, -0.0003]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 92.1MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# import clip\n",
    "# model, clip_preprocess = clip.load(\"ViT-B/32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T17:30:24.841115Z",
     "start_time": "2024-12-29T17:29:52.651358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:01<00:00, 75.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/ubuntu/musical-generative-models-conditioning/data/input/cifar/cat_train_embeds.pt - Embeddings shape: torch.Size([6131, 64]), Labels shape: torch.Size([6131])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 89.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/ubuntu/musical-generative-models-conditioning/data/input/cifar/cat_val_embeds.pt - Embeddings shape: torch.Size([1010, 64]), Labels shape: torch.Size([1010])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:00<00:00, 87.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/ubuntu/musical-generative-models-conditioning/data/input/cifar/dog_train_embeds.pt - Embeddings shape: torch.Size([5421, 64]), Labels shape: torch.Size([5421])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 89.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/ubuntu/musical-generative-models-conditioning/data/input/cifar/dog_val_embeds.pt - Embeddings shape: torch.Size([892, 64]), Labels shape: torch.Size([892])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class FilteredCIFAR10(Dataset):\n",
    "\tdef __init__(self, root, train=True, transform=None, download=False, target_label: int=3):\n",
    "\t\tself.cifar10 = torchvision.datasets.MNIST(\n",
    "\t\t\troot=root,\n",
    "\t\t\ttrain=train,\n",
    "\t\t\ttransform=transform,\n",
    "\t\t\tdownload=download\n",
    "\t\t)\n",
    "\t\tself.indices = []\n",
    "\t\tfor i, (_, label) in enumerate(self.cifar10):\n",
    "\t\t\tif label == target_label:\n",
    "\t\t\t\tself.indices.append(i)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.indices)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treal_idx = self.indices[idx]\n",
    "\t\timage, label = self.cifar10[real_idx]\n",
    "\t\treturn image, label\n",
    "\n",
    "\n",
    "def compute_and_save_embeddings(loader, model, device, save_path):\n",
    "\tall_embeddings = []\n",
    "\tall_labels = []\n",
    "\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tfor images, labels in tqdm.tqdm(loader):\n",
    "\t\t\timages = images.to(device)\n",
    "\t\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t\t# Encode images using CLIP\n",
    "\t\t\tembeddings = model.get_features(images)\n",
    "\t\t\t# (Optional) L2-normalize the embeddings\n",
    "\t\t\tembeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\t\t\tall_embeddings.append(embeddings.cpu())\n",
    "\t\t\tall_labels.append(labels.cpu())\n",
    "\n",
    "\tall_embeddings = torch.cat(all_embeddings, dim=0).float()\n",
    "\tall_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "\t# Save to disk\n",
    "\ttorch.save((all_embeddings, all_labels), save_path)\n",
    "\tprint(f\"Saved: {save_path} - Embeddings shape: {all_embeddings.shape}, Labels shape: {all_labels.shape}\")\n",
    "model = LitMNISTModel()\n",
    "model.load_state_dict(torch.load(MODELS_PATH('minist', \"mnist_feature_extractor_weights.pth\")))\n",
    "model = model.to(DEVICE)\n",
    "def embeds_for_label(num:int):\n",
    "\ttr = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "\t])\n",
    "\ttrain_ds = FilteredCIFAR10(INPUT_PATH('cifar'), train=True, transform=tr, target_label=num)\n",
    "\tval_ds = FilteredCIFAR10(INPUT_PATH('cifar'), train=False, transform=tr, target_label=num)\n",
    "\ttrain_loader = DataLoader(train_ds, batch_size=64, shuffle=False)\n",
    "\tval_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "\tcompute_and_save_embeddings(train_loader, model, 'cuda', INPUT_PATH('cifar', f'{\"dog\" if num == 5 else \"cat\"}_train_embeds.pt'))\n",
    "\tcompute_and_save_embeddings(val_loader, model, 'cuda', INPUT_PATH('cifar', f'{\"dog\" if num == 5 else \"cat\"}_val_embeds.pt'))\n",
    "embeds_for_label(3)\n",
    "embeds_for_label(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T18:39:44.948331Z",
     "start_time": "2024-12-29T18:39:35.329837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': tensor([[-0.0040,  0.0136, -0.0177,  ...,  0.0831, -0.0048,  0.0246],\n",
       "         [ 0.0096, -0.0562, -0.0107,  ...,  0.0546, -0.0154, -0.0437],\n",
       "         [ 0.0214, -0.0078, -0.0205,  ...,  0.0582, -0.0021,  0.0043],\n",
       "         ...,\n",
       "         [ 0.0159, -0.0214, -0.0319,  ...,  0.0668, -0.0015,  0.0104],\n",
       "         [-0.0030, -0.0091, -0.0344,  ...,  0.0593,  0.0268,  0.0196],\n",
       "         [-0.0011, -0.0177, -0.0249,  ...,  0.0495, -0.0052,  0.0117]]),\n",
       " 'encoded_music': tensor([[[1668, 1288,  433,  ...,  946, 1404, 1077],\n",
       "          [1714, 1751,  426,  ..., 1462,  637, 1099],\n",
       "          [1530,  360, 1695,  ...,  711, 1070,  453],\n",
       "          [ 814,  745,  204,  ...,  599,  962, 1134]],\n",
       " \n",
       "         [[1966, 1808, 1449,  ..., 1608,  941,  380],\n",
       "          [1664, 1914,  274,  ...,  536,  676,  189],\n",
       "          [ 141, 1907,  863,  ..., 1537,  319, 1342],\n",
       "          [1706,  196, 1487,  ...,  380,  391, 1098]],\n",
       " \n",
       "         [[1964,  839, 1095,  ..., 1474,   87, 1767],\n",
       "          [1166, 1166, 1029,  ...,  872,  652, 1056],\n",
       "          [ 910, 1927, 1931,  ...,  585,  706, 2017],\n",
       "          [1527, 1055, 1879,  ...,   22,  566,  617]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 447, 1407,  866,  ..., 1783, 1247, 1184],\n",
       "          [ 686,  374,  940,  ...,  385, 1395, 1101],\n",
       "          [  43, 1042, 1100,  ..., 1897,   18, 1133],\n",
       "          [1936,  130, 1375,  ..., 1173,  159,  582]],\n",
       " \n",
       "         [[1207, 1826, 1207,  ...,   51,  872, 1292],\n",
       "          [ 228,  193,  298,  ..., 1065, 1828, 1502],\n",
       "          [1428,  786, 1636,  ...,  475,  532, 1123],\n",
       "          [1643, 1312,  281,  ..., 1749,  808,  257]],\n",
       " \n",
       "         [[1153, 2012,  516,  ...,  178, 1370, 1450],\n",
       "          [ 767,   73,   23,  ..., 1731, 1205, 1900],\n",
       "          [1936,  698, 2001,  ...,  304,  685,  304],\n",
       "          [1906,  543, 1163,  ...,  366, 1701,   39]]]),\n",
       " 'prompt': ['In the style of <8bit_0> <8bit_1> <8bit_2> <8bit_3> <8bit_4> <8bit_5>',\n",
       "  'In the style of <8bit_0> <8bit_1> <8bit_2> <8bit_3> <8bit_4> <8bit_5>',\n",
       "  'In the style of <8bit_0> <8bit_1> <8bit_2> <8bit_3> <8bit_4> <8bit_5>',\n",
       "  'In the style of <metal_0> <metal_1> <metal_2> <metal_3> <metal_4> <metal_5>',\n",
       "  'In the style of <8bit_0> <8bit_1> <8bit_2> <8bit_3> <8bit_4> <8bit_5>',\n",
       "  'In the style of <metal_0> <metal_1> <metal_2> <metal_3> <metal_4> <metal_5>',\n",
       "  'In the style of <metal_0> <metal_1> <metal_2> <metal_3> <metal_4> <metal_5>',\n",
       "  'In the style of <metal_0> <metal_1> <metal_2> <metal_3> <metal_4> <metal_5>',\n",
       "  'In the style of <metal_0> <metal_1> <metal_2> <metal_3> <metal_4> <metal_5>',\n",
       "  'In the style of <8bit_0> <8bit_1> <8bit_2> <8bit_3> <8bit_4> <8bit_5>'],\n",
       " 'label': tensor([3, 3, 3, 5, 3, 5, 5, 5, 5, 3])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concepts_db = TextConcepts.from_musicgen(\n",
    "\tmusic_model, TokensProvider(6), ['8bit', 'metal']\n",
    ")\n",
    "dm = ConceptDataModule(concepts_db, 10)\n",
    "dm.setup('a')\n",
    "next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T17:54:02.766222Z",
     "start_time": "2024-12-29T17:53:51.846271Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/1000 [00:08<2:16:43,  8.21s/it]\u001b[A[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "  0%|          | 1/1000 [00:11<3:04:18, 11.07s/it]\n",
      "  0%|          | 0/10 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m out \u001b[38;5;241m=\u001b[39m forward(batch)\n\u001b[1;32m     32\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m compute_cross_entropy(out\u001b[38;5;241m.\u001b[39mlogits, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoded_music\u001b[39m\u001b[38;5;124m'\u001b[39m], out\u001b[38;5;241m.\u001b[39mmask)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/autograd/function.py:276\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def forward(batch):\n",
    "\ttokenized = tokenizer(\n",
    "\t\tbatch['prompt'], return_tensors=\"pt\", padding=True, add_special_tokens=False\n",
    "\t)\n",
    "\tmask = tokenized['attention_mask']\n",
    "\ttext_with_clip = text_model.shared.weight[tokenized['input_ids']]\n",
    "\ttext_with_clip[:, -TOKENS_NUM:, :] = projector(batch['img']).view(-1, TOKENS_NUM, 768)\n",
    "\twith text_conditioner.autocast and torch.set_grad_enabled(True):\n",
    "\t\ttext_emb = text_model(inputs_embeds=text_with_clip, attention_mask=mask).last_hidden_state\n",
    "\ttext_emb = text_conditioner.output_proj(text_emb.to(text_conditioner.output_proj.weight))\n",
    "\ttext_emb = (text_emb * mask.unsqueeze(-1))\n",
    "\twith music_model.autocast:\n",
    "\t\treturn music_model.lm.compute_predictions(batch['encoded_music'], [], {'description': (text_emb, mask)})\n",
    "projector = ClipProjector()\n",
    "optimizer = torch.optim.Adam(projector.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "# for epoch in range(epochs):\n",
    "# \ttotal_loss, num_batches = 0, len(train_dl)\n",
    "# \tfor batch in tqdm.tqdm(train_dl):\n",
    "# \t\toptimizer.zero_grad()\n",
    "# \t\tout = forward(batch)\n",
    "# \t\tloss, _ = compute_cross_entropy(out.logits, batch['encoded_music'], out.mask)\n",
    "# \t\tloss.backward()\n",
    "# \t\toptimizer.step()\n",
    "# \t\ttotal_loss += loss.item()\n",
    "# \n",
    "# \twith torch.no_grad():\n",
    "# \t\ttotal_val_loss, val_num_batches = 0.0, len(val_dl)\n",
    "# \t\tfor val_batch in tqdm.tqdm(val_dl):\n",
    "# \t\t\tval_out = forward(val_batch)\n",
    "# \t\t\tval_loss, _ = compute_cross_entropy(val_out.logits, val_batch['encoded_music'], val_out.mask)\n",
    "\t\t\t\t\n",
    "\t\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen-WI8jtfXt-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
