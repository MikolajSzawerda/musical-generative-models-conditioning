{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.util_tools import compute_cross_entropy, compute_ortho_loss\n",
    "from src.data import ConceptDataModule, get_ds, TokensProvider\n",
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH, MODELS_PATH\n",
    "\n",
    "import audiocraft\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "import torch\n",
    "from torch.onnx.symbolic_opset9 import tensor\n",
    "from torchviz import make_dot\n",
    "import typing as tp\n",
    "from audiocraft.modules.conditioners import ConditioningAttributes\n",
    "import tqdm\n",
    "import torch\n",
    "from audiocraft.data.audio import audio_read, audio_write\n",
    "from audiocraft.data.audio_utils import convert_audio_channels, convert_audio\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.decomposition import PCA\n",
    "from random import shuffle\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "from audioldm_eval.metrics.fad import FrechetAudioDistance\n",
    "import os\n",
    "import contextlib\n",
    "import io\n",
    "import torchaudio\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "def count_directories(path):\n",
    "    import os\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    return sum(os.path.isdir(os.path.join(path, entry)) for entry in os.listdir(path))\n",
    "\n",
    "\n",
    "letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "EXP_NUM = count_directories(LOGS_PATH('textual-inversion')) + 1\n",
    "EXAMPLES_LEN = 5\n",
    "BATCH_SIZE = 5\n",
    "N_TOKENS = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "encodec = model.compression_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tokens = torch.load(INPUT_PATH(\"textual-inversion-v3\", \"data\", \"train\", \"8bit\", \"encoded\", \"music_p15.pt\")).to(DEVICE)\n",
    "tokens = tokens.squeeze()\n",
    "n, k = tokens.shape\n",
    "padding = torch.ones((n, 400 - k), device=tokens.device, dtype=torch.int64) * model.lm.special_token_id\n",
    "tokens = torch.cat((tokens, padding), dim=1).detach()[None]\n",
    "tokens"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tokens = torch.load(INPUT_PATH(\"textual-inversion-v3\", \"data\", \"train\", \"8bit\", \"encoded\", \"music_p15.pt\")).to(DEVICE)\n",
    "tokens = tokens.squeeze()\n",
    "n, k = tokens.shape\n",
    "padding = torch.ones((n, 400 - k), device=tokens.device, dtype=torch.int64) * 0\n",
    "tokens = torch.cat((tokens, padding), dim=1).detach()[None]\n",
    "display_audio(encodec.decode(tokens, None), model.cfg.sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "set([torch.load(INPUT_PATH(\"textual-inversion-v3\", \"data\", \"train\", \"8bit\", \"encoded\", name)).shape[2] for name in\n",
    "     os.listdir(INPUT_PATH(\"textual-inversion-v3\", \"data\", \"train\", \"8bit\", \"encoded\"))])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "concepts = [\"8bit\", \"oim\"]\n",
    "ds = get_ds().filter(lambda x: x['concept'] in concepts)\n",
    "\n",
    "tokens_provider = TokensProvider(10)\n",
    "tokens_by_concept = {concept: list(tokens_provider.get(concept)) for concept in concepts}\n",
    "\n",
    "fad = FrechetAudioDistance(verbose=True, use_pca=True, use_activation=True)\n",
    "dm = ConceptDataModule(ds, tokens_provider, {\n",
    "    \"8bit\": [1, 2],\n",
    "    \"oim\": [2, 3]\n",
    "}, music_len=249, batch_size=10)\n",
    "dm.setup('train')\n",
    "dl = dm.train_dataloader()\n",
    "dl_it = iter(dl)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = next(dl_it)\n",
    "data['concept']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.rand(10, 20, 768)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
