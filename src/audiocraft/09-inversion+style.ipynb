{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernelspec": {
     "display_name": "Audiocraft",
     "language": "python",
     "name": "python3"
    }
   },
   "outputs": [],
   "source": [
    "from src.util_tools import compute_cross_entropy, compute_ortho_loss\n",
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH, MODELS_PATH\n",
    "\n",
    "import audiocraft\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "import torch\n",
    "from torch.onnx.symbolic_opset9 import tensor\n",
    "from torchviz import make_dot\n",
    "import typing as tp\n",
    "from audiocraft.modules.conditioners import ConditioningAttributes\n",
    "import tqdm\n",
    "import torch\n",
    "from audiocraft.data.audio import audio_read, audio_write\n",
    "from audiocraft.data.audio_utils import convert_audio_channels, convert_audio\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.decomposition import PCA\n",
    "from random import shuffle\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "from audioldm_eval.metrics.fad import FrechetAudioDistance\n",
    "import os\n",
    "import contextlib\n",
    "import io\n",
    "import torchaudio\n",
    "import random\n",
    "def count_directories(path):\n",
    "    import os\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    return sum(os.path.isdir(os.path.join(path, entry)) for entry in os.listdir(path))\n",
    "\n",
    "letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "EXP_NUM = count_directories(LOGS_PATH('textual-inversion'))+1\n",
    "EXAMPLES_LEN = 5\n",
    "BATCH_SIZE = 5\n",
    "N_TOKENS = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicGen.get_pretrained('facebook/musicgen-style')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_generation_params(\n",
    "    duration=5, # generate 8 seconds, can go up to 30\n",
    "    use_sampling=True, \n",
    "    top_k=250,\n",
    "    cfg_coef=3., # Classifier Free Guidance coefficient \n",
    "    cfg_coef_beta=8., # double CFG is necessary for text-and-style conditioning\n",
    "                   # Beta in the double CFG formula. between 1 and 9. When set to 1 it is equivalent to normal CFG. \n",
    "                   # When we increase this parameter, the text condition is pushed. See the bottom of https://musicgenstyle.github.io/ \n",
    "                   # to better understand the effects of the double CFG coefficients. \n",
    ")\n",
    "\n",
    "model.set_style_conditioner_params(\n",
    "    eval_q=2, # integer between 1 and 6\n",
    "              # eval_q is the level of quantization that passes\n",
    "              # through the conditioner. When low, the models adheres less to the \n",
    "              # audio conditioning\n",
    "    excerpt_length=4.5, # the length in seconds that is taken by the model in the provided excerpt, can be                 \n",
    "                       # between 1.5 and 4.5 seconds but it has to be shortest to the length of the provided conditioning\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept=\"8bit\"\n",
    "examples = os.listdir(INPUT_PATH('textual-inversion-v3', 'data', 'valid', f'{concept}', 'fad'))\n",
    "random.shuffle(examples)\n",
    "songs = []\n",
    "for fname in tqdm.tqdm(examples[:5]):\n",
    "    melody, sr = audio_read(INPUT_PATH('textual-inversion-v3', 'data', 'valid', f'{concept}', 'fad', fname), pad=True, duration=5)\n",
    "    songs.append(melody[0][None].expand(3, -1, -1))\n",
    "songs = torch.cat(songs, dim=0)\n",
    "results = model.generate_with_chroma([None]*len(songs), songs, sr, progress=True)\n",
    "for a_idx in range(results.shape[0]):\n",
    "    music = results[a_idx].cpu()\n",
    "    music = music/np.max(np.abs(music.numpy()))\n",
    "    path = OUTPUT_PATH(\"musigen-style\", concept, 'temp', f'music_p{a_idx}')\n",
    "    audio_write(path, music, model.cfg.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"polar-totem-39\"\n",
    "torch.load(MODELS_PATH(\"textual-inversion-v3\", f\"{exp_name}-best.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_new_tokens(tokenizer, weights, data_by_concept):\n",
    "    for concept, data in data_by_concept.items():\n",
    "        assert len(data['tokens']) == data['embeds'].shape[0]\n",
    "        for i, token in enumerate(data['tokens']):\n",
    "            tokenizer.add_tokens(token)\n",
    "            token_id = tokenizer.convert_tokens_to_ids([token])[0]\n",
    "            with torch.no_grad():\n",
    "                weights[token_id] = data['embeds'][i]\n",
    "class TokensProvider:\n",
    "    def __init__(self, num: int):\n",
    "        self.num = num\n",
    "    \n",
    "    def get(self, base: str):\n",
    "        return [f'<{base}_{x}>' for x in range(self.num)]\n",
    "    \n",
    "    def get_str(self, base: str):\n",
    "        return ' '.join(self.get(base))\n",
    "concept=\"8bit\"\n",
    "exp_name = \"polar-totem-39\"\n",
    "embedings = torch.load(MODELS_PATH(\"textual-inversion-v3\", f\"{exp_name}-best.pt\"))\n",
    "model = MusicGen.get_pretrained(\"facebook/musicgen-small\")\n",
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=5\n",
    ")\n",
    "tokens_provider = TokensProvider(embedings[concept]['embeds'].shape[0])\n",
    "text_conditioner=list(model.lm.condition_provider.conditioners.values())[0]\n",
    "tokenizer=text_conditioner.t5_tokenizer\n",
    "text_model=text_conditioner.t5\n",
    "\n",
    "append_new_tokens(tokenizer, text_model.shared.weight, {concept: {\n",
    "    'tokens': tokens_provider.get(concept),\n",
    "    'embeds': embedings[concept]['embeds']\n",
    "    }})\n",
    "text_model.resize_token_embeddings(len(tokenizer))\n",
    "ti_res=model.generate([f'In the style of {tokens_provider.get_str(concept)}']*5)\n",
    "for a_idx in range(results.shape[0]):\n",
    "    music = results[a_idx].cpu()\n",
    "    music = music/np.max(np.abs(music.numpy()))\n",
    "    path = OUTPUT_PATH(\"musigen-style\", concept, 'temp', f'music_p{a_idx}')\n",
    "    audio_write(path, music, model.cfg.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fad = FrechetAudioDistance(verbose=True, use_pca=True, use_activation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_fad = lambda path: list(fad.score(INPUT_PATH('textual-inversion-v3', 'data', 'valid', f'{concept}', 'fad'), path, recalculate=True).values())[0]*1e-5\n",
    "print(\"STYLE:\", calc_fad(OUTPUT_PATH(\"musigen-style\", concept, 'temp')))\n",
    "print(\"TI:\", calc_fad(OUTPUT_PATH(\"textual-inversion-v3\", concept, 'temp')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen-ufgTm-Qc-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
