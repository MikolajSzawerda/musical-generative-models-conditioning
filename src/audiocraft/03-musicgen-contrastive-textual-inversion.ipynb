{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from scipy.stats import describe\n",
    "# from src.util_tools import compute_contrastive_loss_with_labels\n",
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH\n",
    "from torch.nn import functional as F\n",
    "import audiocraft\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "import torch\n",
    "from gradio.cli.commands.components.publish import colors\n",
    "from omegaconf import DictConfig\n",
    "from torch import set_grad_enabled\n",
    "from torch.onnx.symbolic_opset9 import tensor\n",
    "from torchviz import make_dot\n",
    "import typing as tp\n",
    "from audiocraft.modules.conditioners import ConditioningAttributes\n",
    "import tqdm\n",
    "import torch\n",
    "from audiocraft.data.audio import audio_read, audio_write\n",
    "from audiocraft.data.audio_utils import convert_audio_channels, convert_audio\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.decomposition import PCA\n",
    "from random import shuffle\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, Dataset\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from pytorch_metric_learning.losses import NTXentLoss\n",
    "\n",
    "EXP_NUM = 1\n",
    "EXAMPLES_LEN = 5\n",
    "BATCH_SIZE = 5\n",
    "EXAMPLES_NUM = 20\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=EXAMPLES_LEN\n",
    ")"
   ],
   "id": "3a9fc075f3e17905",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "new_token = \"S*\"\n",
    "text_conditioner = list(model.lm.condition_provider.conditioners.values())[0]\n",
    "tokenizer = text_conditioner.t5_tokenizer\n",
    "text_model = text_conditioner.t5.to(DEVICE)\n",
    "lm = model.lm\n",
    "new_token_id = tokenizer.convert_tokens_to_ids(new_token)\n",
    "\n",
    "if tokenizer.add_tokens([new_token]) == 1:\n",
    "    text_model.resize_token_embeddings(len(tokenizer))\n",
    "    with torch.no_grad():\n",
    "        text_model.shared.weight[new_token_id] = text_model.shared.weight.mean(dim=0)"
   ],
   "id": "92917138e5d5f107",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_music(song: str, idxs):\n",
    "    songs = []\n",
    "    for p in idxs:\n",
    "        music, sr = audio_read(INPUT_PATH('textual-inversion', 'contrastive', song, f'p{p}.wav'), duration=EXAMPLES_LEN,\n",
    "                               pad=True)\n",
    "        music = music[None]\n",
    "        songs.append(convert_audio(music, sr, 32000, 1))\n",
    "    with torch.no_grad():\n",
    "        encoded_music, _ = model.compression_model.encode(torch.concatenate(songs).to(DEVICE))\n",
    "    return encoded_music\n",
    "\n",
    "\n",
    "def load_music_to_pt():\n",
    "    return torch.concatenate([get_music(range(i, i + 10)) for i in range(0, 230, 10)])\n",
    "\n",
    "# torch.save(get_music('music_1', range(25)), INPUT_PATH('textual-inversion', 'contrastive', 'music_1.pt'))\n",
    "# torch.save(get_music('music_10', range(26)), INPUT_PATH('textual-inversion', 'contrastive', 'music_10.pt'))\n",
    "\n",
    "# dl = lambda x, s: DataLoader(x, batch_size=BATCH_SIZE, shuffle=s, pin_memory=True if torch.cuda.is_available() else False)\n",
    "# torch.save(load_music_to_pt(), INPUT_PATH('textual-inversion', '8bit', 'encoded.pt'))"
   ],
   "id": "e3ad461536d77f8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "music_1_ts = torch.load(INPUT_PATH('textual-inversion', 'contrastive', 'music_1.pt'))[:EXAMPLES_NUM, :, :]\n",
    "music_2_ts = torch.load(INPUT_PATH('textual-inversion', 'contrastive', 'music_10.pt'))[:EXAMPLES_NUM, :, :]"
   ],
   "id": "909db1771f611735",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "happy_adventurous_prompts = [\n",
    "    \"In the style of S*, a bright and cheerful journey begins\",\n",
    "    \"S* audio of an energetic adventure through sunny lands\",\n",
    "    \"A vibrant melody in the style of S*, setting off on a great quest\",\n",
    "    \"Start of an epic journey, upbeat audio of S*\",\n",
    "    \"S* journey's intro with lively, adventurous sounds\",\n",
    "    \"An exciting adventure unfolds in the style of S*\",\n",
    "    \"Audio of S*, upbeat and joyful for a new journey\",\n",
    "    \"A happy, adventurous intro, S* soundtrack\",\n",
    "    \"S* soundtrack for the first steps of a thrilling adventure\",\n",
    "    \"Uplifting audio in the style of S* for a bold new quest\",\n",
    "    \"Adventurous intro, in the energetic style of S*\",\n",
    "    \"S* audio capturing the thrill of setting out on a journey\",\n",
    "    \"Bright, adventurous music in the style of S*\",\n",
    "    \"The start of a grand journey with S* in a joyful tone\",\n",
    "    \"Audio of S* filled with excitement and adventure\",\n",
    "    \"An exciting journey begins, S* theme music\",\n",
    "    \"Upbeat and energetic, in the style of S*\",\n",
    "    \"Audio of S*, inspiring an adventurous beginning\",\n",
    "    \"S* style melody, bright and full of anticipation\",\n",
    "    \"Joyful, adventurous sounds in the style of S*\"\n",
    "]\n",
    "slow_melancholic_prompts = [\n",
    "    \"In the style of S*, a slow and melancholic melody\",\n",
    "    \"S* audio of a somber and reflective tone\",\n",
    "    \"A sorrowful tune in the style of S*, slow and gentle\",\n",
    "    \"Melancholic audio of S*, capturing a deep sadness\",\n",
    "    \"S* in a slow, reflective style, evoking nostalgia\",\n",
    "    \"A slow, melancholic theme, in the style of S*\",\n",
    "    \"Audio of S*, quiet and mournful\",\n",
    "    \"S* soundtrack, slow and filled with melancholy\",\n",
    "    \"A melancholic, emotional piece in the style of S*\",\n",
    "    \"In the reflective style of S*, a slow, sad melody\",\n",
    "    \"S* audio expressing deep emotion and sorrow\",\n",
    "    \"Slow and melancholic, in the style of S*\",\n",
    "    \"S* soundtrack evoking a deep sense of longing\",\n",
    "    \"A gentle, melancholic piece, in the style of S*\",\n",
    "    \"In the style of S*, a slow melody of quiet sadness\",\n",
    "    \"Audio of S*, portraying a melancholy mood\",\n",
    "    \"A somber theme in the style of S*, slow and emotional\",\n",
    "    \"S* audio filled with gentle sorrow\",\n",
    "    \"Melancholic and slow, in the reflective style of S*\",\n",
    "    \"S* soundtrack, soft and melancholic, filled with longing\"\n",
    "]\n"
   ],
   "id": "ee0376e6ff5a9a3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res = tokenizer(happy_adventurous_prompts + slow_melancholic_prompts, return_tensors='pt', padding=True,\n",
    "                add_special_tokens=False)\n",
    "ids, mask = res['input_ids'], res['attention_mask']"
   ],
   "id": "c1e006172a440f70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "out_1 = lm.compute_predictions(music_1_ts[:3], [ConditioningAttributes({'description': \"S*\"})])\n",
    "out_2 = lm.compute_predictions(music_2_ts[:3], [ConditioningAttributes({'description': \"S*\"})])"
   ],
   "id": "ee475d66c5f9c209",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def flatten(x, m, k):\n",
    "    m_k = m[:, k, ...].contiguous().view(-1)\n",
    "    x_k = x[:, k, ...].contiguous().view(-1, x.size(-1))\n",
    "    return x_k[m_k]\n",
    "\n",
    "\n",
    "a = flatten(out_1.logits, out_1.mask, 0)\n",
    "b = flatten(out_2.logits, out_2.mask, 0)"
   ],
   "id": "d343169c5fd38da1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d6161afada043e67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a[0, 0, ...] * b[0, 0, ...]",
   "id": "fdc9e668b9499cf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ContrastiveRow = namedtuple('ContrastiveRow', 'song_1 prompt_1 song_2 prompt_2 example_type')\n",
    "\n",
    "\n",
    "class PairMusic(Dataset):\n",
    "    def __init__(self, ts_1: torch.tensor, ts_2: torch.tensor, pos=10, neg=20):\n",
    "        self.ts_1 = ts_1\n",
    "        self.ts_2 = ts_2\n",
    "        self.pos = pos\n",
    "        self.neg = neg\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2 * self.pos + self.neg - 1\n",
    "\n",
    "    def __getitem__(self, idx) -> ContrastiveRow:\n",
    "        s1, p1, s2, p2 = random.sample(range(EXAMPLES_NUM), 4)\n",
    "        if idx < self.pos:\n",
    "            return ContrastiveRow(\n",
    "                self.ts_1[s1],\n",
    "                (ids[p1], mask[p1]),\n",
    "                self.ts_1[s2],\n",
    "                (ids[p2], mask[p2]),\n",
    "                1\n",
    "            )\n",
    "        if idx < 2 * self.pos:\n",
    "            return ContrastiveRow(\n",
    "                self.ts_2[s1],\n",
    "                (ids[p1], mask[p1]),\n",
    "                self.ts_2[s2],\n",
    "                (ids[p2], mask[p2]),\n",
    "                1\n",
    "            )\n",
    "        return ContrastiveRow(\n",
    "            self.ts_1[s1],\n",
    "            (ids[p1], mask[p1]),\n",
    "            self.ts_2[s2],\n",
    "            (ids[p2], mask[p2]),\n",
    "            0\n",
    "        )\n",
    "\n",
    "\n",
    "dl = lambda x, s: DataLoader(x, batch_size=BATCH_SIZE, shuffle=s,\n",
    "                             pin_memory=True if torch.cuda.is_available() else False)\n",
    "ds = PairMusic(music_1_ts, music_2_ts)\n",
    "train_ds, val_ds = random_split(ds, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "train_dl, val_dl = dl(train_ds, True), dl(val_ds, False)\n",
    "batch = next(iter(train_dl))"
   ],
   "id": "4a51bb96f7452431",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "new_token = \"S*\"\n",
    "text_conditioner = list(model.lm.condition_provider.conditioners.values())[0]\n",
    "tokenizer = text_conditioner.t5_tokenizer\n",
    "text_model = text_conditioner.t5.to(DEVICE)\n",
    "new_token_id = tokenizer.convert_tokens_to_ids(new_token)\n",
    "\n",
    "if tokenizer.add_tokens([new_token]) == 1:\n",
    "    text_model.resize_token_embeddings(len(tokenizer))\n",
    "    with torch.no_grad():\n",
    "        text_model.shared.weight[new_token_id] = text_model.shared.weight.mean(dim=0)"
   ],
   "id": "ad74c56d4d1d9957",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "song_1, prompt_1, song_2, prompt_2, label = batch\n",
    "mask = torch.concat([prompt_1[1], prompt_2[1]])\n",
    "input_ids = torch.concat([prompt_1[0], prompt_2[0]])\n",
    "song = torch.concat([song_1, song_2])\n",
    "with text_conditioner.autocast and torch.set_grad_enabled(True):\n",
    "    embeds = text_model(**{'input_ids': input_ids, 'attention_mask': mask}).last_hidden_state\n",
    "embeds = text_conditioner.output_proj(embeds.to(text_conditioner.output_proj.weight))\n",
    "embeds = (embeds * mask.unsqueeze(-1))\n",
    "with model.autocast:\n",
    "    out = lm.compute_predictions(song, [], {'description': (embeds, mask)})"
   ],
   "id": "7c4bebfdf7152dea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "out.mask.shape",
   "id": "e115774a1691fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import typing as tp\n",
    "\n",
    "\n",
    "def compute_contrastive_loss_with_labels(\n",
    "        logits: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        temperature: float = 0.5\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\tCompute contrastive loss between logits of song_1 and song_2 using labels to determine\n",
    "\twhether a pair is positive or negative.\n",
    "\n",
    "\tArgs:\n",
    "\t\tlogits (torch.Tensor): Stacked logits for song_1 and song_2 of shape [2 * B, K, T, card].\n",
    "\t\t\t\t\t\t\t   logits[:B] are for song_1, logits[B:] are for song_2.\n",
    "\t\tlabels (torch.Tensor): Binary labels (1 for positive, 0 for negative) of shape [B].\n",
    "\t\t\t\t\t\t\t   These labels determine if the pair (song_1, song_2) is a positive or negative pair.\n",
    "\t\ttemperature (float): Temperature scaling for contrastive loss.\n",
    "\n",
    "\tReturns:\n",
    "\t\tcontrastive_loss (torch.Tensor): The computed contrastive loss.\n",
    "\t\"\"\"\n",
    "    B, K, T, _ = logits.shape\n",
    "    num_examples = logits.shape[0] // 2  # Assuming first half is for song_1, second half is for song_2\n",
    "    assert logits.shape[0] % 2 == 0, \"Logits should be stacked for song_1 and song_2.\"\n",
    "    cl = torch.zeros([], device=logits.device)\n",
    "    for k in range(K):\n",
    "        logits_k = logits[:, k, ...].contiguous().view(-1, logits.size(-1))\n",
    "        mask_k = mask[:, k, ...].contiguous().view(-1)\n",
    "        cl_logits = logits_k[mask_k]\n",
    "        logits_s1 = cl_logits[:num_examples]\n",
    "        logits_s2 = cl_logits[num_examples:]\n",
    "        logits_s1 = F.normalize(logits_s1, dim=1)\n",
    "        logits_s2 = F.normalize(logits_s2, dim=1)\n",
    "        cos_sim = F.cosine_similarity(logits_s1, logits_s2)\n",
    "        pos_loss = (1 - labels) * torch.pow(cos_sim, 2)\n",
    "        neg_loss = labels * torch.pow(torch.clamp(1.0 - cos_sim, min=0.0), 2)\n",
    "        cl += torch.mean(pos_loss + neg_loss)\n",
    "\n",
    "    return cl / K\n",
    "\n",
    "\n",
    "compute_contrastive_loss_with_labels(out.logits, label)"
   ],
   "id": "3320b195c6ad440f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
