{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data import ConceptDataset, get_ds\n",
    "from src.util_tools import compute_cross_entropy, compute_ortho_loss\n",
    "\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "import tqdm\n",
    "import pytorch_lightning as L\n",
    "from datasets import load_dataset\n",
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH, MODELS_PATH\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.data.audio import audio_read, audio_write\n",
    "from audioldm_eval.metrics.fad import FrechetAudioDistance\n",
    "\n",
    "EXAMPLES_LEN = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszawerda/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Using cache found in /Users/mszawerda/.cache/torch/hub/harritaylor_torchvggish_master\n"
     ]
    }
   ],
   "source": [
    "music_model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "music_model.set_generation_params(\n",
    "\tuse_sampling=True,\n",
    "\ttop_k=250,\n",
    "\tduration=EXAMPLES_LEN\n",
    ")\n",
    "text_conditioner=list(music_model.lm.condition_provider.conditioners.values())[0]\n",
    "tokenizer=text_conditioner.t5_tokenizer\n",
    "text_model=text_conditioner.t5\n",
    "fad = FrechetAudioDistance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH, MODELS_PATH, RAW_PATH\n",
    "import torch\n",
    "import os\n",
    "from datasets import Audio, load_dataset\n",
    "from datasets import load_dataset\n",
    "from random import choice\n",
    "import tqdm\n",
    "\n",
    "train_desc = [\n",
    "    \"the sound of %s\",\n",
    "    \"pure %s audio\",\n",
    "    \"the recorded %s sound\",\n",
    "    \"%s audio sample\",\n",
    "    \"recording of %s\",\n",
    "    \"high fidelity %s audio\",\n",
    "    \"%s sound clip\",\n",
    "    \"audio of %s\",\n",
    "    \"captured %s sound\",\n",
    "    \"%s audio recording\",\n",
    "    \"%s recording capture\",\n",
    "    \"audio file of %s\",\n",
    "    \"isolated %s sound\",\n",
    "    \"distinct %s recording\",\n",
    "    \"quality %s audio file\",\n",
    "    \"high-definition %s sound\",\n",
    "    \"the sound recording of %s\",\n",
    "    \"audio segment of %s\",\n",
    "    \"raw %s audio\",\n",
    "    \"%s sound snippet\",\n",
    "    \"%s audio track\",\n",
    "    \"%s sound fragment\",\n",
    "    \"audio recording for %s\",\n",
    "    \"sound capture of %s\",\n",
    "    \"%s audio file sample\",\n",
    "    \"the isolated %s recording\",\n",
    "    \"%s recorded audio\",\n",
    "    \"pure capture of %s\",\n",
    "    \"audio segment capture of %s\",\n",
    "    \"the sample of %s audio\",\n",
    "    \"the sound file of %s\",\n",
    "    \"full recording of %s\",\n",
    "    \"%s audio archive\",\n",
    "    \"%s sound collection\",\n",
    "    \"captured audio of %s\",\n",
    "    \"%s isolated sound file\",\n",
    "    \"the audio snippet of %s\",\n",
    "    \"clean audio of %s\",\n",
    "    \"%s audio capture\",\n",
    "    \"%s sound extract\"\n",
    "]\n",
    "\n",
    "val_desc = [\n",
    "    \"audio capture of %s\",\n",
    "    \"%s sound recording\",\n",
    "    \"pristine %s audio\",\n",
    "    \"clear %s recording\",\n",
    "    \"the audio of %s\",\n",
    "    \"%s audio sample capture\",\n",
    "    \"the recorded sound of %s\",\n",
    "    \"sample of %s audio\",\n",
    "    \"%s audio segment\",\n",
    "    \"recorded audio of %s\",\n",
    "    \"%s audio\",\n",
    "    \"distinct sound of %s\",\n",
    "    \"unprocessed %s audio\",\n",
    "    \"%s recording\",\n",
    "    \"high clarity %s sound\",\n",
    "    \"%s recording sample\",\n",
    "    \"audio portion of %s\",\n",
    "    \"sampled audio of %s\",\n",
    "    \"unfiltered %s audio\",\n",
    "    \"audio segment for %s\",\n",
    "    \"clip of %s audio\",\n",
    "    \"the audio snippet for %s\",\n",
    "    \"audio portion of %s\",\n",
    "    \"%s recorded segment\",\n",
    "    \"sampled sound of %s\",\n",
    "    \"%s captured in audio\",\n",
    "    \"audio excerpt of %s\",\n",
    "    \"full audio capture of %s\",\n",
    "    \"%s sound archive\",\n",
    "    \"audio track of %s\",\n",
    "    \"%s in sound format\",\n",
    "    \"%s sound recording sample\",\n",
    "    \"captured file of %s sound\",\n",
    "    \"the distinct sound of %s\",\n",
    "    \"high quality %s sound sample\",\n",
    "    \"%s in captured audio\",\n",
    "    \"pure audio of %s\",\n",
    "    \"clean capture of %s audio\",\n",
    "    \"recorded file of %s\",\n",
    "    \"audio format of %s\"\n",
    "]\n",
    "\n",
    "def get_ds():\n",
    "    return load_dataset('json', data_files={\n",
    "                'valid': INPUT_PATH('textual-inversion-v3', 'metadata_val.json'),\n",
    "                'train': INPUT_PATH('textual-inversion-v3', 'metadata_train.json')\n",
    "                })\n",
    "\n",
    "class TokensProvider:\n",
    "    def __init__(self, num: int):\n",
    "        self.num = num\n",
    "    \n",
    "    def get(self, base: str):\n",
    "        return [f'<{base}_{x}>' for x in range(self.num)]\n",
    "    \n",
    "    def get_str(self, base: str):\n",
    "        return ' '.join(self.get(base))\n",
    "\n",
    "class PromptProvider:\n",
    "    def __init__(self, prompts_template):\n",
    "        self.template = prompts_template\n",
    "    \n",
    "    def get(self, *args):\n",
    "        return choice(self.template) % args\n",
    "\n",
    "class ConceptDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ds, tokenizer, split: str, sr: int=32000, tokens_num: int=1, music_len: int=100):\n",
    "        self.ds = ds\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if self.ds.cache_files:\n",
    "            self.base_dir = os.path.dirname(self.ds.cache_files[0][\"filename\"])\n",
    "        else:\n",
    "            raise ValueError(\"No cache files found in the dataset\")\n",
    "        self.base_dir = INPUT_PATH('textual-inversion-v3')\n",
    "\n",
    "        if split == 'valid':\n",
    "            def map_path(x):\n",
    "                x['audio'] = os.path.join(self.base_dir, x['audio_path'])\n",
    "                return x\n",
    "            self.ds = self.ds.map(map_path).cast_column('audio', Audio(sampling_rate=sr))\n",
    "\n",
    "        self.encoded = {}\n",
    "        self.tokens_num = tokens_num\n",
    "        self.prompter = PromptProvider(val_desc if split == 'valid' else train_desc)\n",
    "        self.tokens_provider = TokensProvider(tokens_num)\n",
    "        self.music_len = music_len\n",
    "        self.split = split\n",
    "        self.concpets = None\n",
    "        self.tokenized_prompts = {}\n",
    "        self.tokens_ids = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def _random_slice(self, tensor):\n",
    "        n, k = tensor.shape\n",
    "        \n",
    "        if self.music_len <= k:\n",
    "            start_col = torch.randint(0, k - self.music_len + 1, (1,)).item()\n",
    "            return tensor[:, start_col:start_col + self.music_len]\n",
    "        else:\n",
    "            padding = torch.zeros((n, self.music_len - k), device=tensor.device)\n",
    "            return torch.cat((tensor, padding), dim=1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ds[idx]\n",
    "        path = row['encoded_path']\n",
    "        if path not in self.encoded:\n",
    "            self.encoded[path] = torch.load(os.path.join(self.base_dir, path)).squeeze()\n",
    "        y = path.replace(\"\\\\\", \"\").split('/')[2]\n",
    "        if y not in self.tokens_ids:\n",
    "            self.tokens_ids[y] = self.tokenizer.convert_tokens_to_ids(list(self.tokens_provider.get(y)))\n",
    "        prompt = self.prompter.get(self.tokens_provider.get_str(y))\n",
    "        # if prompt not in self.tokenized_prompts:\n",
    "        #     self.tokenized_prompts[prompt] = self.tokenizer([prompt], return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "        return {\n",
    "            'encoded_music': self._random_slice(self.encoded[path]),\n",
    "            'prompt': prompt,\n",
    "            'new_token_ids': self.tokens_ids[y],\n",
    "            # **({} if self.split == 'train' else \n",
    "            #     {\n",
    "            #         'audio': row['audio']['array']\n",
    "            #     })\n",
    "        }\n",
    "    \n",
    "    def _get_concepts(self):\n",
    "        unique_values = set()\n",
    "        def collect_unique(batch):\n",
    "            unique_values.update([x.replace(\"\\\\\", \"\").split('/')[2] for x in batch['audio_path']])\n",
    "        self.ds.map(collect_unique, batched=True, batch_size=1000)\n",
    "        return unique_values\n",
    "    \n",
    "    def get_concepts(self):\n",
    "        if self.concpets is None:\n",
    "            self.concpets = self._get_concepts()\n",
    "        return self.concpets\n",
    "    \n",
    "    def get_new_tokens(self) -> set[str]:\n",
    "        res = set()\n",
    "        for concept in self.get_concepts():\n",
    "            res.update(self.tokens_provider.get(concept))\n",
    "        return res\n",
    "    \n",
    "    def get_new_tokens_ids(self) -> set[int]:\n",
    "        return self.tokenizer.convert_tokens_to_ids(self.get_new_tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptDataModule(L.LightningDataModule):\n",
    "    def __init__(self, tokenizer, tokens_num:int=5, music_len: int = 255, batch_size: int = 5):\n",
    "        super().__init__()\n",
    "        self.tokens_num = tokens_num\n",
    "        self.music_len = music_len\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        get_ds()\n",
    "    \n",
    "    def setup(self, stage: str):\n",
    "        print(stage)\n",
    "        ds = get_ds()\n",
    "        self.train_ds = ConceptDataset(ds['train'], self.tokenizer, 'train', tokens_num=self.tokens_num, music_len=self.music_len)\n",
    "        self.val_ds = ConceptDataset(ds['valid'], self.tokenizer, 'valid', tokens_num=self.tokens_num, music_len=self.music_len)\n",
    "    \n",
    "    def get_new_tokens(self)->list[str]:\n",
    "        new_tokens = self.train_ds.get_new_tokens()\n",
    "        new_tokens.update(self.val_ds.get_new_tokens())\n",
    "        return list(new_tokens)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        prompts = [item['prompt'] for item in batch]\n",
    "        tokenized_prompts = self.tokenizer(prompts, return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "        for i, item in enumerate(batch):\n",
    "            item['tokenized_prompt'] = {\n",
    "                'input_ids': tokenized_prompts['input_ids'][i],\n",
    "                'attention_mask': tokenized_prompts['attention_mask'][i],\n",
    "            }\n",
    "        collated_batch = default_collate(batch)\n",
    "        collated_batch['batch_tokens'] = torch.unique(torch.cat(collated_batch['new_token_ids']))\n",
    "        return collated_batch\n",
    "    \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, collate_fn=self.collate_fn, num_workers=os.cpu_count())\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, collate_fn=self.collate_fn, num_workers=os.cpu_count())\n",
    "\n",
    "# dm = ConceptDataModule(tokenizer)\n",
    "# dm.setup('a')\n",
    "\n",
    "# if tokenizer.add_tokens(dm.get_new_tokens()) > 0:\n",
    "#     text_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# dl=dm.val_dataloader()\n",
    "# for batch in tqdm.tqdm(dl):\n",
    "#     x, y = batch['encoded_music'], batch['prompt']\n",
    "# dl=dm.train_dataloader()\n",
    "# for batch in tqdm.tqdm(dl):\n",
    "#     x, y = batch['encoded_music'], batch['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszawerda/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Using cache found in /Users/mszawerda/.cache/torch/hub/harritaylor_torchvggish_master\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mszawerda/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/mszawerda/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:72: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerFn.FITTING\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9180aa326fcf4c3981930724301298f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                    | Type           | Params | Mode\n",
      "------------------------------------------------------------------\n",
      "0 | text_model              | T5EncoderModel | 109 M  | eval\n",
      "1 | music_model_conditioner | T5Conditioner  | 787 K  | eval\n",
      "------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "441.664   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "225       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9840c233fd142dab75467f9510d1de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszawerda/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m tb_logger \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mloggers\u001b[38;5;241m.\u001b[39mTensorBoardLogger(LOGS_PATH, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtextual-inversion-v3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[GenEvalCallback([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupbeat\u001b[39m\u001b[38;5;124m'\u001b[39m], fad)], enable_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, logger\u001b[38;5;241m=\u001b[39mtb_logger)\n\u001b[0;32m--> 110\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1052\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1049\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:113\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_run_start()\n\u001b[1;32m    115\u001b[0m data_fetcher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:231\u001b[0m, in \u001b[0;36m_EvaluationLoop.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m combined_loader\u001b[38;5;241m.\u001b[39mlimits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_batches\n\u001b[1;32m    230\u001b[0m data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# creates the iterator inside the fetcher\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# add the previous `fetched` value to properly track `is_last_batch` with no prefetching\u001b[39;00m\n\u001b[1;32m    234\u001b[0m data_fetcher\u001b[38;5;241m.\u001b[39mfetched \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mready\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:104\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_PrefetchDataFetcher\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;66;03m# ignore pre-fetching, it's not necessary\u001b[39;00m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:51\u001b[0m, in \u001b[0;36m_DataFetcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_DataFetcher\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:351\u001b[0m, in \u001b[0;36mCombinedLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m _SUPPORTED_MODES[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miterator\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    350\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflattened, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits)\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m iterator\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:155\u001b[0m, in \u001b[0;36m_Sequential.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_current_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:173\u001b[0m, in \u001b[0;36m_Sequential._load_current_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_current_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# Load a single DataLoader, prevents multiple sets of workers from starting unnecessarily\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterables):\n\u001b[0;32m--> 173\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;66;03m# No more iterables to step through, return an empty list\u001b[39;00m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:386\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1039\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1032\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1039\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:193\u001b[0m, in \u001b[0;36mreduce_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    190\u001b[0m storage \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39m_typed_storage()\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mis_leaf:\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCowardly refusing to serialize non-leaf tensor which requires_grad, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msince autograd does not support crossing process boundaries.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you just want to transfer the data, call detach() on the tensor \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore serializing (e.g., putting it on the queue).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    200\u001b[0m check_serializing_named_tensor(tensor)\n\u001b[1;32m    201\u001b[0m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mhooks\u001b[38;5;241m.\u001b[39mwarn_if_has_hooks(tensor)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue)."
     ]
    }
   ],
   "source": [
    "class TransformerTextualInversion(L.LightningModule):\n",
    "    def __init__(self, text_model, tokenizer, music_model, music_model_conditioner, lamda_tokens, \n",
    "                 grad_amplify: float=10.0,\n",
    "                 entropy_alpha: float=1e1,\n",
    "                 ortho_alpha: float=1e-2\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        # self.save_hyperparameters()  # Saves all init arguments to the checkpoint\n",
    "        self.grad_amplify = grad_amplify\n",
    "        self.entropy_alpha = entropy_alpha\n",
    "        self.ortho_alpha = ortho_alpha\n",
    "\n",
    "        self.text_model = text_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.music_model = music_model\n",
    "        self.fetch_new_tokens = lamda_tokens\n",
    "        self.music_model_conditioner = music_model_conditioner\n",
    "\n",
    "        \n",
    "    def _init_text_model(self, new_tokens):\n",
    "        if tokenizer.add_tokens(new_tokens) > 0:\n",
    "            self.text_model.resize_token_embeddings(len(tokenizer))\n",
    "        new_token_ids = tokenizer.convert_tokens_to_ids(new_tokens)\n",
    "        with torch.no_grad():\n",
    "            for new_token_id in new_token_ids:\n",
    "                text_model.shared.weight[new_token_id] = text_model.shared.weight.mean(dim=0)\n",
    "        def zero_existing_emb(grad):\n",
    "            mask = torch.zeros_like(grad)\n",
    "            for new_token_id in new_token_ids:\n",
    "                mask[new_token_id] = self.grad_amplify\n",
    "            return grad * mask\n",
    "\n",
    "        self.text_model.shared.weight.register_hook(zero_existing_emb)\n",
    "        \n",
    "    def on_train_start(self):\n",
    "        self._init_text_model(self.fetch_new_tokens())\n",
    "\n",
    "    def forward(self, encoded_music, tokenized_prompt):\n",
    "        mask = tokenized_prompt['attention_mask']\n",
    "        with self.music_model_conditioner.autocast and torch.set_grad_enabled(True):\n",
    "            x_e = self.text_model(**tokenized_prompt).last_hidden_state\n",
    "        x_e = self.music_model_conditioner.output_proj(x_e.to(self.music_model_conditioner.output_proj.weight))\n",
    "        x_e = (x_e * mask.unsqueeze(-1))\n",
    "        with self.music_model.autocast:\n",
    "            x = self.music_model.lm.compute_predictions(encoded_music, [], {'description': (x_e, mask)})\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        music, prompt = batch['encoded_music'], batch['tokenized_prompt']\n",
    "        out = self(music, prompt)\n",
    "        ce_loss, _ = compute_cross_entropy(out.logits, music, out.mask)\n",
    "        ortho_loss = compute_ortho_loss(self.text_model.shared.weight[batch['batch_tokens']])\n",
    "        loss = self.entropy_alpha * ce_loss + self.ortho_alpha * ortho_loss\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"ortho_loss\", ortho_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        music, prompt = batch['encoded_music'], batch['tokenized_prompt']\n",
    "        out = self(music, prompt)\n",
    "        val_loss, _ = compute_cross_entropy(out.logits, music, out.mask)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Optimizer and learning rate scheduler setup\n",
    "        optimizer = Adam([self.text_model.shared.weight], lr=1e-1)\n",
    "        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return ([optimizer], \n",
    "                []\n",
    "                )\n",
    "class GenEvalCallback(L.Callback):\n",
    "    def __init__(self, generation_concepts, fad, n_epochs=2):\n",
    "        super().__init__()\n",
    "        self.n_epochs = n_epochs\n",
    "        self.concepts = generation_concepts\n",
    "        self.fad = fad\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if (trainer.current_epoch+1) % self.n_epochs == 0:\n",
    "            print(f\"Generation time at epoch {trainer.current_epoch + 1}\")\n",
    "            concept = self.concepts[0]\n",
    "            response = pl_module.music_model.generate([f'In the style of {TokensProvider(5).get_str(concept)}']*3)\n",
    "            for a_idx in range(response.shape[0]):\n",
    "                music = response[a_idx].cpu()\n",
    "                music = music/np.max(np.abs(music.numpy()))\n",
    "                path = OUTPUT_PATH(\"textual-inversion-v3\", concept, 'temp', f'music_p{a_idx}')\n",
    "                audio_write(path, music, pl_module.music_model.cfg.sample_rate)\n",
    "                pl_module.logger.experiment.add_audio(f\"{concept} {a_idx}\", music, trainer.global_step, sample_rate=pl_module.music_model.cfg.sample_rate)\n",
    "            with contextlib.redirect_stdout(io.StringIO()):\n",
    "                fd_score = self.fad.score(INPUT_PATH('textual-inversion-v3', 'data', 'valid', f'{concept}', 'audio'), OUTPUT_PATH(\"textual-inversion-v3\", concept, 'temp'))\n",
    "                os.remove(OUTPUT_PATH(\"textual-inversion-v3\", concept, 'temp_fad_feature_cache.npy'))\n",
    "                pl_module.log('FAD', list(fd_score.values())[0], trainer.global_step)\n",
    "\n",
    "music_model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "music_model.set_generation_params(\n",
    "\tuse_sampling=True,\n",
    "\ttop_k=250,\n",
    "\tduration=EXAMPLES_LEN\n",
    ")\n",
    "text_conditioner=list(music_model.lm.condition_provider.conditioners.values())[0]\n",
    "tokenizer=text_conditioner.t5_tokenizer\n",
    "text_model=text_conditioner.t5\n",
    "fad = FrechetAudioDistance()\n",
    "\n",
    "dm = ConceptDataModule(tokenizer, music_len=255)\n",
    "model = TransformerTextualInversion(text_model, tokenizer, music_model, text_conditioner, lambda: dm.get_new_tokens())\n",
    "tb_logger = L.loggers.TensorBoardLogger(LOGS_PATH, name='textual-inversion-v3')\n",
    "trainer = L.Trainer(accelerator='cpu', callbacks=[GenEvalCallback(['cluster_0'], fad)], enable_checkpointing=False, logger=tb_logger)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen-YATmys4o-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
