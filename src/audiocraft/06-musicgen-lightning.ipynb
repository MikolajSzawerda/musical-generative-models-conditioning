{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# from src.data import ConceptDataset, get_ds\n",
    "from src.util_tools import compute_cross_entropy, compute_ortho_loss\n",
    "\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "import tqdm\n",
    "import pytorch_lightning as L\n",
    "from datasets import load_dataset\n",
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH, MODELS_PATH\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.data.audio import audio_read, audio_write\n",
    "from audioldm_eval.metrics.fad import FrechetAudioDistance\n",
    "\n",
    "EXAMPLES_LEN = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "music_model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "music_model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=EXAMPLES_LEN\n",
    ")\n",
    "text_conditioner = list(music_model.lm.condition_provider.conditioners.values())[0]\n",
    "tokenizer = text_conditioner.t5_tokenizer\n",
    "text_model = text_conditioner.t5\n",
    "fad = FrechetAudioDistance()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH, MODELS_PATH, RAW_PATH\n",
    "import torch\n",
    "import os\n",
    "from datasets import Audio, load_dataset\n",
    "from datasets import load_dataset\n",
    "from random import choice\n",
    "import tqdm\n",
    "\n",
    "train_desc = [\n",
    "    \"the sound of %s\",\n",
    "    \"pure %s audio\",\n",
    "    \"the recorded %s sound\",\n",
    "    \"%s audio sample\",\n",
    "    \"recording of %s\",\n",
    "    \"high fidelity %s audio\",\n",
    "    \"%s sound clip\",\n",
    "    \"audio of %s\",\n",
    "    \"captured %s sound\",\n",
    "    \"%s audio recording\",\n",
    "    \"%s recording capture\",\n",
    "    \"audio file of %s\",\n",
    "    \"isolated %s sound\",\n",
    "    \"distinct %s recording\",\n",
    "    \"quality %s audio file\",\n",
    "    \"high-definition %s sound\",\n",
    "    \"the sound recording of %s\",\n",
    "    \"audio segment of %s\",\n",
    "    \"raw %s audio\",\n",
    "    \"%s sound snippet\",\n",
    "    \"%s audio track\",\n",
    "    \"%s sound fragment\",\n",
    "    \"audio recording for %s\",\n",
    "    \"sound capture of %s\",\n",
    "    \"%s audio file sample\",\n",
    "    \"the isolated %s recording\",\n",
    "    \"%s recorded audio\",\n",
    "    \"pure capture of %s\",\n",
    "    \"audio segment capture of %s\",\n",
    "    \"the sample of %s audio\",\n",
    "    \"the sound file of %s\",\n",
    "    \"full recording of %s\",\n",
    "    \"%s audio archive\",\n",
    "    \"%s sound collection\",\n",
    "    \"captured audio of %s\",\n",
    "    \"%s isolated sound file\",\n",
    "    \"the audio snippet of %s\",\n",
    "    \"clean audio of %s\",\n",
    "    \"%s audio capture\",\n",
    "    \"%s sound extract\"\n",
    "]\n",
    "\n",
    "val_desc = [\n",
    "    \"audio capture of %s\",\n",
    "    \"%s sound recording\",\n",
    "    \"pristine %s audio\",\n",
    "    \"clear %s recording\",\n",
    "    \"the audio of %s\",\n",
    "    \"%s audio sample capture\",\n",
    "    \"the recorded sound of %s\",\n",
    "    \"sample of %s audio\",\n",
    "    \"%s audio segment\",\n",
    "    \"recorded audio of %s\",\n",
    "    \"%s audio\",\n",
    "    \"distinct sound of %s\",\n",
    "    \"unprocessed %s audio\",\n",
    "    \"%s recording\",\n",
    "    \"high clarity %s sound\",\n",
    "    \"%s recording sample\",\n",
    "    \"audio portion of %s\",\n",
    "    \"sampled audio of %s\",\n",
    "    \"unfiltered %s audio\",\n",
    "    \"audio segment for %s\",\n",
    "    \"clip of %s audio\",\n",
    "    \"the audio snippet for %s\",\n",
    "    \"audio portion of %s\",\n",
    "    \"%s recorded segment\",\n",
    "    \"sampled sound of %s\",\n",
    "    \"%s captured in audio\",\n",
    "    \"audio excerpt of %s\",\n",
    "    \"full audio capture of %s\",\n",
    "    \"%s sound archive\",\n",
    "    \"audio track of %s\",\n",
    "    \"%s in sound format\",\n",
    "    \"%s sound recording sample\",\n",
    "    \"captured file of %s sound\",\n",
    "    \"the distinct sound of %s\",\n",
    "    \"high quality %s sound sample\",\n",
    "    \"%s in captured audio\",\n",
    "    \"pure audio of %s\",\n",
    "    \"clean capture of %s audio\",\n",
    "    \"recorded file of %s\",\n",
    "    \"audio format of %s\"\n",
    "]\n",
    "\n",
    "\n",
    "def get_ds():\n",
    "    return load_dataset('json', data_files={\n",
    "        'valid': INPUT_PATH('textual-inversion-v3', 'metadata_val.json'),\n",
    "        'train': INPUT_PATH('textual-inversion-v3', 'metadata_train.json')\n",
    "    })\n",
    "\n",
    "\n",
    "class TokensProvider:\n",
    "    def __init__(self, num: int):\n",
    "        self.num = num\n",
    "\n",
    "    def get(self, base: str):\n",
    "        return [f'<{base}_{x}>' for x in range(self.num)]\n",
    "\n",
    "    def get_str(self, base: str):\n",
    "        return ' '.join(self.get(base))\n",
    "\n",
    "\n",
    "class PromptProvider:\n",
    "    def __init__(self, prompts_template):\n",
    "        self.template = prompts_template\n",
    "\n",
    "    def get(self, *args):\n",
    "        return choice(self.template) % args\n",
    "\n",
    "\n",
    "class ConceptDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ds, tokenizer, split: str, sr: int = 32000, tokens_num: int = 1, music_len: int = 100):\n",
    "        self.ds = ds\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if self.ds.cache_files:\n",
    "            self.base_dir = os.path.dirname(self.ds.cache_files[0][\"filename\"])\n",
    "        else:\n",
    "            raise ValueError(\"No cache files found in the dataset\")\n",
    "        self.base_dir = INPUT_PATH('textual-inversion-v3')\n",
    "\n",
    "        if split == 'valid':\n",
    "            def map_path(x):\n",
    "                x['audio'] = os.path.join(self.base_dir, x['audio_path'])\n",
    "                return x\n",
    "\n",
    "            self.ds = self.ds.map(map_path).cast_column('audio', Audio(sampling_rate=sr))\n",
    "\n",
    "        self.encoded = {}\n",
    "        self.tokens_num = tokens_num\n",
    "        self.prompter = PromptProvider(val_desc if split == 'valid' else train_desc)\n",
    "        self.tokens_provider = TokensProvider(tokens_num)\n",
    "        self.music_len = music_len\n",
    "        self.split = split\n",
    "        self.concpets = None\n",
    "        self.tokenized_prompts = {}\n",
    "        self.tokens_ids = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def _random_slice(self, tensor):\n",
    "        n, k = tensor.shape\n",
    "\n",
    "        if self.music_len <= k:\n",
    "            start_col = torch.randint(0, k - self.music_len + 1, (1,)).item()\n",
    "            return tensor[:, start_col:start_col + self.music_len]\n",
    "        else:\n",
    "            padding = torch.zeros((n, self.music_len - k), device=tensor.device)\n",
    "            return torch.cat((tensor, padding), dim=1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ds[idx]\n",
    "        path = row['encoded_path']\n",
    "        if path not in self.encoded:\n",
    "            self.encoded[path] = torch.load(os.path.join(self.base_dir, path)).squeeze()\n",
    "        y = path.replace(\"\\\\\", \"\").split('/')[2]\n",
    "        if y not in self.tokens_ids:\n",
    "            self.tokens_ids[y] = self.tokenizer.convert_tokens_to_ids(list(self.tokens_provider.get(y)))\n",
    "        prompt = self.prompter.get(self.tokens_provider.get_str(y))\n",
    "        # if prompt not in self.tokenized_prompts:\n",
    "        #     self.tokenized_prompts[prompt] = self.tokenizer([prompt], return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "        return {\n",
    "            'encoded_music': self._random_slice(self.encoded[path]),\n",
    "            'prompt': prompt,\n",
    "            'new_token_ids': self.tokens_ids[y],\n",
    "            # **({} if self.split == 'train' else \n",
    "            #     {\n",
    "            #         'audio': row['audio']['array']\n",
    "            #     })\n",
    "        }\n",
    "\n",
    "    def _get_concepts(self):\n",
    "        unique_values = set()\n",
    "\n",
    "        def collect_unique(batch):\n",
    "            unique_values.update([x.replace(\"\\\\\", \"\").split('/')[2] for x in batch['audio_path']])\n",
    "\n",
    "        self.ds.map(collect_unique, batched=True, batch_size=1000)\n",
    "        return unique_values\n",
    "\n",
    "    def get_concepts(self):\n",
    "        if self.concpets is None:\n",
    "            self.concpets = self._get_concepts()\n",
    "        return self.concpets\n",
    "\n",
    "    def get_new_tokens(self) -> set[str]:\n",
    "        res = set()\n",
    "        for concept in self.get_concepts():\n",
    "            res.update(self.tokens_provider.get(concept))\n",
    "        return res\n",
    "\n",
    "    def get_new_tokens_ids(self) -> set[int]:\n",
    "        return self.tokenizer.convert_tokens_to_ids(self.get_new_tokens())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class ConceptDataModule(L.LightningDataModule):\n",
    "    def __init__(self, tokenizer, tokens_num: int = 5, music_len: int = 255, batch_size: int = 5):\n",
    "        super().__init__()\n",
    "        self.tokens_num = tokens_num\n",
    "        self.music_len = music_len\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        get_ds()\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        print(stage)\n",
    "        ds = get_ds()\n",
    "        self.train_ds = ConceptDataset(ds['train'], self.tokenizer, 'train', tokens_num=self.tokens_num,\n",
    "                                       music_len=self.music_len)\n",
    "        self.val_ds = ConceptDataset(ds['valid'], self.tokenizer, 'valid', tokens_num=self.tokens_num,\n",
    "                                     music_len=self.music_len)\n",
    "\n",
    "    def get_new_tokens(self) -> list[str]:\n",
    "        new_tokens = self.train_ds.get_new_tokens()\n",
    "        new_tokens.update(self.val_ds.get_new_tokens())\n",
    "        return list(new_tokens)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        prompts = [item['prompt'] for item in batch]\n",
    "        tokenized_prompts = self.tokenizer(prompts, return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "        for i, item in enumerate(batch):\n",
    "            item['tokenized_prompt'] = {\n",
    "                'input_ids': tokenized_prompts['input_ids'][i],\n",
    "                'attention_mask': tokenized_prompts['attention_mask'][i],\n",
    "            }\n",
    "        collated_batch = default_collate(batch)\n",
    "        collated_batch['batch_tokens'] = torch.unique(torch.cat(collated_batch['new_token_ids']))\n",
    "        return collated_batch\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, collate_fn=self.collate_fn,\n",
    "                          num_workers=os.cpu_count())\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, collate_fn=self.collate_fn,\n",
    "                          num_workers=os.cpu_count())\n",
    "\n",
    "# dm = ConceptDataModule(tokenizer)\n",
    "# dm.setup('a')\n",
    "\n",
    "# if tokenizer.add_tokens(dm.get_new_tokens()) > 0:\n",
    "#     text_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# dl=dm.val_dataloader()\n",
    "# for batch in tqdm.tqdm(dl):\n",
    "#     x, y = batch['encoded_music'], batch['prompt']\n",
    "# dl=dm.train_dataloader()\n",
    "# for batch in tqdm.tqdm(dl):\n",
    "#     x, y = batch['encoded_music'], batch['prompt']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TransformerTextualInversion(L.LightningModule):\n",
    "    def __init__(self, text_model, tokenizer, music_model, music_model_conditioner, lamda_tokens,\n",
    "                 grad_amplify: float = 10.0,\n",
    "                 entropy_alpha: float = 1e1,\n",
    "                 ortho_alpha: float = 1e-2\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        # self.save_hyperparameters()  # Saves all init arguments to the checkpoint\n",
    "        self.grad_amplify = grad_amplify\n",
    "        self.entropy_alpha = entropy_alpha\n",
    "        self.ortho_alpha = ortho_alpha\n",
    "\n",
    "        self.text_model = text_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.music_model = music_model\n",
    "        self.fetch_new_tokens = lamda_tokens\n",
    "        self.music_model_conditioner = music_model_conditioner\n",
    "\n",
    "    def _init_text_model(self, new_tokens):\n",
    "        if tokenizer.add_tokens(new_tokens) > 0:\n",
    "            self.text_model.resize_token_embeddings(len(tokenizer))\n",
    "        new_token_ids = tokenizer.convert_tokens_to_ids(new_tokens)\n",
    "        with torch.no_grad():\n",
    "            for new_token_id in new_token_ids:\n",
    "                text_model.shared.weight[new_token_id] = text_model.shared.weight.mean(dim=0)\n",
    "\n",
    "        def zero_existing_emb(grad):\n",
    "            mask = torch.zeros_like(grad)\n",
    "            for new_token_id in new_token_ids:\n",
    "                mask[new_token_id] = self.grad_amplify\n",
    "            return grad * mask\n",
    "\n",
    "        self.text_model.shared.weight.register_hook(zero_existing_emb)\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self._init_text_model(self.fetch_new_tokens())\n",
    "\n",
    "    def forward(self, encoded_music, tokenized_prompt):\n",
    "        mask = tokenized_prompt['attention_mask']\n",
    "        with self.music_model_conditioner.autocast and torch.set_grad_enabled(True):\n",
    "            x_e = self.text_model(**tokenized_prompt).last_hidden_state\n",
    "        x_e = self.music_model_conditioner.output_proj(x_e.to(self.music_model_conditioner.output_proj.weight))\n",
    "        x_e = (x_e * mask.unsqueeze(-1))\n",
    "        with self.music_model.autocast:\n",
    "            x = self.music_model.lm.compute_predictions(encoded_music, [], {'description': (x_e, mask)})\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        music, prompt = batch['encoded_music'], batch['tokenized_prompt']\n",
    "        out = self(music, prompt)\n",
    "        ce_loss, _ = compute_cross_entropy(out.logits, music, out.mask)\n",
    "        ortho_loss = compute_ortho_loss(self.text_model.shared.weight[batch['batch_tokens']])\n",
    "        loss = self.entropy_alpha * ce_loss + self.ortho_alpha * ortho_loss\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"ortho_loss\", ortho_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        music, prompt = batch['encoded_music'], batch['tokenized_prompt']\n",
    "        out = self(music, prompt)\n",
    "        val_loss, _ = compute_cross_entropy(out.logits, music, out.mask)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Optimizer and learning rate scheduler setup\n",
    "        optimizer = Adam([self.text_model.shared.weight], lr=1e-1)\n",
    "        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return ([optimizer],\n",
    "                []\n",
    "                )\n",
    "\n",
    "\n",
    "class GenEvalCallback(L.Callback):\n",
    "    def __init__(self, generation_concepts, fad, n_epochs=2):\n",
    "        super().__init__()\n",
    "        self.n_epochs = n_epochs\n",
    "        self.concepts = generation_concepts\n",
    "        self.fad = fad\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if (trainer.current_epoch + 1) % self.n_epochs == 0:\n",
    "            print(f\"Generation time at epoch {trainer.current_epoch + 1}\")\n",
    "            concept = self.concepts[0]\n",
    "            response = pl_module.music_model.generate([f'In the style of {TokensProvider(5).get_str(concept)}'] * 3)\n",
    "            for a_idx in range(response.shape[0]):\n",
    "                music = response[a_idx].cpu()\n",
    "                music = music / np.max(np.abs(music.numpy()))\n",
    "                path = OUTPUT_PATH(\"textual-inversion-v3\", concept, 'temp', f'music_p{a_idx}')\n",
    "                audio_write(path, music, pl_module.music_model.cfg.sample_rate)\n",
    "                pl_module.logger.experiment.add_audio(f\"{concept} {a_idx}\", music, trainer.global_step,\n",
    "                                                      sample_rate=pl_module.music_model.cfg.sample_rate)\n",
    "            with contextlib.redirect_stdout(io.StringIO()):\n",
    "                fd_score = self.fad.score(INPUT_PATH('textual-inversion-v3', 'data', 'valid', f'{concept}', 'audio'),\n",
    "                                          OUTPUT_PATH(\"textual-inversion-v3\", concept, 'temp'))\n",
    "                os.remove(OUTPUT_PATH(\"textual-inversion-v3\", concept, 'temp_fad_feature_cache.npy'))\n",
    "                pl_module.log('FAD', list(fd_score.values())[0], trainer.global_step)\n",
    "\n",
    "\n",
    "music_model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "music_model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=EXAMPLES_LEN\n",
    ")\n",
    "text_conditioner = list(music_model.lm.condition_provider.conditioners.values())[0]\n",
    "tokenizer = text_conditioner.t5_tokenizer\n",
    "text_model = text_conditioner.t5\n",
    "fad = FrechetAudioDistance()\n",
    "\n",
    "dm = ConceptDataModule(tokenizer, music_len=255)\n",
    "model = TransformerTextualInversion(text_model, tokenizer, music_model, text_conditioner, lambda: dm.get_new_tokens())\n",
    "tb_logger = L.loggers.TensorBoardLogger(LOGS_PATH, name='textual-inversion-v3')\n",
    "trainer = L.Trainer(accelerator='cpu', callbacks=[GenEvalCallback(['cluster_0'], fad)], enable_checkpointing=False,\n",
    "                    logger=tb_logger)\n",
    "trainer.fit(model, dm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Audiocraft",
   "language": "python",
   "name": "audiocraft_lab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
