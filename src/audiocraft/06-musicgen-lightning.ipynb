{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data import ConceptDataset, get_ds\n",
    "from src.util_tools import compute_cross_entropy, compute_ortho_loss\n",
    "\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "import tqdm\n",
    "import pytorch_lightning as L\n",
    "from datasets import load_dataset\n",
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH, MODELS_PATH\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.data.audio import audio_read, audio_write\n",
    "from audioldm_eval.metrics.fad import FrechetAudioDistance\n",
    "\n",
    "EXAMPLES_LEN = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH, MODELS_PATH, RAW_PATH\n",
    "import torch\n",
    "import os\n",
    "from datasets import Audio, load_dataset\n",
    "from datasets import load_dataset\n",
    "from random import choice\n",
    "import tqdm\n",
    "\n",
    "train_desc = [\n",
    "    \"the sound of %s\",\n",
    "    \"pure %s audio\",\n",
    "    \"the recorded %s sound\",\n",
    "    \"%s audio sample\",\n",
    "    \"recording of %s\",\n",
    "    \"high fidelity %s audio\",\n",
    "    \"%s sound clip\",\n",
    "    \"audio of %s\",\n",
    "    \"captured %s sound\",\n",
    "    \"%s audio recording\",\n",
    "    \"%s recording capture\",\n",
    "    \"audio file of %s\",\n",
    "    \"isolated %s sound\",\n",
    "    \"distinct %s recording\",\n",
    "    \"quality %s audio file\",\n",
    "    \"high-definition %s sound\",\n",
    "    \"the sound recording of %s\",\n",
    "    \"audio segment of %s\",\n",
    "    \"raw %s audio\",\n",
    "    \"%s sound snippet\",\n",
    "    \"%s audio track\",\n",
    "    \"%s sound fragment\",\n",
    "    \"audio recording for %s\",\n",
    "    \"sound capture of %s\",\n",
    "    \"%s audio file sample\",\n",
    "    \"the isolated %s recording\",\n",
    "    \"%s recorded audio\",\n",
    "    \"pure capture of %s\",\n",
    "    \"audio segment capture of %s\",\n",
    "    \"the sample of %s audio\",\n",
    "    \"the sound file of %s\",\n",
    "    \"full recording of %s\",\n",
    "    \"%s audio archive\",\n",
    "    \"%s sound collection\",\n",
    "    \"captured audio of %s\",\n",
    "    \"%s isolated sound file\",\n",
    "    \"the audio snippet of %s\",\n",
    "    \"clean audio of %s\",\n",
    "    \"%s audio capture\",\n",
    "    \"%s sound extract\"\n",
    "]\n",
    "\n",
    "val_desc = [\n",
    "    \"audio capture of %s\",\n",
    "    \"%s sound recording\",\n",
    "    \"pristine %s audio\",\n",
    "    \"clear %s recording\",\n",
    "    \"the audio of %s\",\n",
    "    \"%s audio sample capture\",\n",
    "    \"the recorded sound of %s\",\n",
    "    \"sample of %s audio\",\n",
    "    \"%s audio segment\",\n",
    "    \"recorded audio of %s\",\n",
    "    \"%s audio\",\n",
    "    \"distinct sound of %s\",\n",
    "    \"unprocessed %s audio\",\n",
    "    \"%s recording\",\n",
    "    \"high clarity %s sound\",\n",
    "    \"%s recording sample\",\n",
    "    \"audio portion of %s\",\n",
    "    \"sampled audio of %s\",\n",
    "    \"unfiltered %s audio\",\n",
    "    \"audio segment for %s\",\n",
    "    \"clip of %s audio\",\n",
    "    \"the audio snippet for %s\",\n",
    "    \"audio portion of %s\",\n",
    "    \"%s recorded segment\",\n",
    "    \"sampled sound of %s\",\n",
    "    \"%s captured in audio\",\n",
    "    \"audio excerpt of %s\",\n",
    "    \"full audio capture of %s\",\n",
    "    \"%s sound archive\",\n",
    "    \"audio track of %s\",\n",
    "    \"%s in sound format\",\n",
    "    \"%s sound recording sample\",\n",
    "    \"captured file of %s sound\",\n",
    "    \"the distinct sound of %s\",\n",
    "    \"high quality %s sound sample\",\n",
    "    \"%s in captured audio\",\n",
    "    \"pure audio of %s\",\n",
    "    \"clean capture of %s audio\",\n",
    "    \"recorded file of %s\",\n",
    "    \"audio format of %s\"\n",
    "]\n",
    "\n",
    "def get_ds():\n",
    "    return load_dataset('json', data_files={\n",
    "                'valid': INPUT_PATH('textual-inversion-v2', 'metadata_val.json'),\n",
    "                'train': INPUT_PATH('textual-inversion-v2', 'metadata_train.json')\n",
    "                })\n",
    "\n",
    "class TokensProvider:\n",
    "    def __init__(self, num: int):\n",
    "        self.num = num\n",
    "    \n",
    "    def get(self, base: str):\n",
    "        return [f'<{base}_{x}>' for x in range(self.num)]\n",
    "    \n",
    "    def get_str(self, base: str):\n",
    "        return ' '.join(self.get(base))\n",
    "\n",
    "class PromptProvider:\n",
    "    def __init__(self, prompts_template):\n",
    "        self.template = prompts_template\n",
    "    \n",
    "    def get(self, *args):\n",
    "        return choice(self.template) % args\n",
    "\n",
    "class ConceptDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ds, tokenizer, split: str, sr: int=32000, tokens_num: int=1, music_len: int=100):\n",
    "        self.ds = ds\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if self.ds.cache_files:\n",
    "            self.base_dir = os.path.dirname(self.ds.cache_files[0][\"filename\"])\n",
    "        else:\n",
    "            raise ValueError(\"No cache files found in the dataset\")\n",
    "        self.base_dir = INPUT_PATH('textual-inversion-v2')\n",
    "\n",
    "        if split == 'valid':\n",
    "            def map_path(x):\n",
    "                x['audio'] = os.path.join(self.base_dir, x['audio_path'])\n",
    "                return x\n",
    "            self.ds = self.ds.map(map_path).cast_column('audio', Audio(sampling_rate=sr))\n",
    "\n",
    "        self.encoded = {}\n",
    "        self.tokens_num = tokens_num\n",
    "        self.prompter = PromptProvider(val_desc if split == 'valid' else train_desc)\n",
    "        self.tokens_provider = TokensProvider(tokens_num)\n",
    "        self.music_len = music_len\n",
    "        self.split = split\n",
    "        self.concpets = None\n",
    "        self.tokenized_prompts = {}\n",
    "        self.tokens_ids = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def _random_slice(self, tensor):\n",
    "        n, k = tensor.shape\n",
    "        \n",
    "        if self.music_len <= k:\n",
    "            start_col = torch.randint(0, k - self.music_len + 1, (1,)).item()\n",
    "            return tensor[:, start_col:start_col + self.music_len]\n",
    "        else:\n",
    "            padding = torch.zeros((n, self.music_len - k), device=tensor.device)\n",
    "            return torch.cat((tensor, padding), dim=1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ds[idx]\n",
    "        path = row['encoded_path']\n",
    "        if path not in self.encoded:\n",
    "            self.encoded[path] = torch.load(os.path.join(self.base_dir, path)).squeeze()\n",
    "        y = path.replace(\"\\\\\", \"\").split('/')[2]\n",
    "        if y not in self.tokens_ids:\n",
    "            self.tokens_ids[y] = self.tokenizer.convert_tokens_to_ids(list(self.tokens_provider.get(y)))\n",
    "        prompt = self.prompter.get(self.tokens_provider.get_str(y))\n",
    "        # if prompt not in self.tokenized_prompts:\n",
    "        #     self.tokenized_prompts[prompt] = self.tokenizer([prompt], return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "        return {\n",
    "            'encoded_music': self._random_slice(self.encoded[path]),\n",
    "            'prompt': prompt,\n",
    "            'new_token_ids': self.tokens_ids[y],\n",
    "            **({} if self.split == 'train' else \n",
    "                {\n",
    "                    'audio': row['audio']['array']\n",
    "                })\n",
    "        }\n",
    "    \n",
    "    def _get_concepts(self):\n",
    "        unique_values = set()\n",
    "        def collect_unique(batch):\n",
    "            unique_values.update([x.replace(\"\\\\\", \"\").split('/')[2] for x in batch['audio_path']])\n",
    "        self.ds.map(collect_unique, batched=True, batch_size=1000)\n",
    "        return unique_values\n",
    "    \n",
    "    def get_concepts(self):\n",
    "        if self.concpets is None:\n",
    "            self.concpets = self._get_concepts()\n",
    "        return self.concpets\n",
    "    \n",
    "    def get_new_tokens(self) -> set[str]:\n",
    "        res = set()\n",
    "        for concept in self.get_concepts():\n",
    "            res.update(self.tokens_provider.get(concept))\n",
    "        return res\n",
    "    \n",
    "    def get_new_tokens_ids(self) -> set[int]:\n",
    "        return self.tokenizer.convert_tokens_to_ids(self.get_new_tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptDataModule(L.LightningDataModule):\n",
    "    def __init__(self, tokenizer, tokens_num:int=5, music_len: int = 255, batch_size: int = 5):\n",
    "        super().__init__()\n",
    "        self.tokens_num = tokens_num\n",
    "        self.music_len = music_len\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        get_ds()\n",
    "    \n",
    "    def setup(self, stage: str):\n",
    "        print(stage)\n",
    "        ds = get_ds()\n",
    "        self.train_ds = ConceptDataset(ds['train'], self.tokenizer, 'train', tokens_num=self.tokens_num, music_len=self.music_len)\n",
    "        self.val_ds = ConceptDataset(ds['valid'], self.tokenizer, 'valid', tokens_num=self.tokens_num, music_len=self.music_len)\n",
    "    \n",
    "    def get_new_tokens(self)->list[str]:\n",
    "        new_tokens = self.train_ds.get_new_tokens()\n",
    "        new_tokens.update(self.val_ds.get_new_tokens())\n",
    "        return list(new_tokens)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        prompts = [item['prompt'] for item in batch]\n",
    "        tokenized_prompts = self.tokenizer(prompts, return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "        for i, item in enumerate(batch):\n",
    "            item['tokenized_prompt'] = {\n",
    "                'input_ids': tokenized_prompts['input_ids'][i],\n",
    "                'attention_mask': tokenized_prompts['attention_mask'][i],\n",
    "            }\n",
    "        collated_batch = default_collate(batch)\n",
    "        collated_batch['batch_tokens'] = torch.unique(torch.cat(collated_batch['new_token_ids']))\n",
    "        return collated_batch\n",
    "    \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "# dm = ConceptDataModule(tokenizer)\n",
    "# dm.setup('a')\n",
    "\n",
    "# if tokenizer.add_tokens(dm.get_new_tokens()) > 0:\n",
    "#     text_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# dl=dm.val_dataloader()\n",
    "# for batch in tqdm.tqdm(dl):\n",
    "#     x, y, a = batch['encoded_music'], batch['prompt'], batch['audio']\n",
    "# dl=dm.train_dataloader()\n",
    "# for batch in tqdm.tqdm(dl):\n",
    "#     x, y = batch['encoded_music'], batch['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszawerda/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Using cache found in /Users/mszawerda/.cache/torch/hub/harritaylor_torchvggish_master\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mszawerda/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/mszawerda/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:72: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerFn.FITTING\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1720d278ebc446083acbb289ead0c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                    | Type           | Params | Mode\n",
      "------------------------------------------------------------------\n",
      "0 | text_model              | T5EncoderModel | 109 M  | eval\n",
      "1 | music_model_conditioner | T5Conditioner  | 787 K  | eval\n",
      "------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "441.664   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "225       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370ffaa6903545b6952705858ea4ca24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszawerda/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszawerda/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/mszawerda/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11561fe4dd045d7a17a4c67b484f236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a08d82d405043f3a09f8420aa5fcc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd3222ff17344a5af5d821bc79fa930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0091fdb0e1437cabe9fa38a5642eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:04<?, ?it/s]\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:141\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:295\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    293\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_model_zero_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:142\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_run_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:254\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_evaluation_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m logged_outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs, []  \u001b[38;5;66;03m# free memory\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:333\u001b[0m, in \u001b[0;36m_EvaluationLoop._on_evaluation_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 333\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, hook_name)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:218\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 91\u001b[0m, in \u001b[0;36mGenEvalCallback.on_validation_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mredirect_stdout(io\u001b[38;5;241m.\u001b[39mStringIO()):\n\u001b[0;32m---> 91\u001b[0m     fd_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUT_PATH\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtextual-inversion-v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mconcept\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_PATH\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtextual-inversion-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(OUTPUT_PATH(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextual-inversion-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, concept, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_fad_feature_cache.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/audioldm_eval/metrics/fad.py:171\u001b[0m, in \u001b[0;36mFrechetAudioDistance.score\u001b[0;34m(self, background_dir, eval_dir, store_embds, limit_num, recalculate)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(fad_target_folder_cache) \u001b[38;5;129;01mor\u001b[39;00m recalculate):\n\u001b[0;32m--> 171\u001b[0m     embds_eval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(fad_target_folder_cache, embds_eval)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/audioldm_eval/metrics/fad.py:70\u001b[0m, in \u001b[0;36mFrechetAudioDistance.get_embeddings\u001b[0;34m(self, x, sr, limit_num)\u001b[0m\n\u001b[1;32m     69\u001b[0m embd_lst \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_audio_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m): \n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/audioldm_eval/metrics/fad.py:56\u001b[0m, in \u001b[0;36mFrechetAudioDistance.load_audio_data\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data to RAM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(outputloader):\n\u001b[1;32m     57\u001b[0m     data_list\u001b[38;5;241m.\u001b[39mappend((batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m16000\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m     success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m tb_logger \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mloggers\u001b[38;5;241m.\u001b[39mTensorBoardLogger(LOGS_PATH, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtextual-inversion-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[GenEvalCallback([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupbeat\u001b[39m\u001b[38;5;124m'\u001b[39m], fad)], enable_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, logger\u001b[38;5;241m=\u001b[39mtb_logger)\n\u001b[0;32m--> 110\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/musicgen-YATmys4o-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "class TransformerTextualInversion(L.LightningModule):\n",
    "    def __init__(self, text_model, tokenizer, music_model, music_model_conditioner, lamda_tokens, \n",
    "                 grad_amplify: float=10.0,\n",
    "                 entropy_alpha: float=1e1,\n",
    "                 ortho_alpha: float=1e-2\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        # self.save_hyperparameters()  # Saves all init arguments to the checkpoint\n",
    "        self.grad_amplify = grad_amplify\n",
    "        self.entropy_alpha = entropy_alpha\n",
    "        self.ortho_alpha = ortho_alpha\n",
    "\n",
    "        self.text_model = text_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.music_model = music_model\n",
    "        self.fetch_new_tokens = lamda_tokens\n",
    "        self.music_model_conditioner = music_model_conditioner\n",
    "\n",
    "        \n",
    "    def _init_text_model(self, new_tokens):\n",
    "        if tokenizer.add_tokens(new_tokens) > 0:\n",
    "            self.text_model.resize_token_embeddings(len(tokenizer))\n",
    "        new_token_ids = tokenizer.convert_tokens_to_ids(new_tokens)\n",
    "        with torch.no_grad():\n",
    "            for new_token_id in new_token_ids:\n",
    "                text_model.shared.weight[new_token_id] = text_model.shared.weight.mean(dim=0)\n",
    "        def zero_existing_emb(grad):\n",
    "            mask = torch.zeros_like(grad)\n",
    "            for new_token_id in new_token_ids:\n",
    "                mask[new_token_id] = self.grad_amplify\n",
    "            return grad * mask\n",
    "\n",
    "        self.text_model.shared.weight.register_hook(zero_existing_emb)\n",
    "        \n",
    "    def on_train_start(self):\n",
    "        self._init_text_model(self.fetch_new_tokens())\n",
    "\n",
    "    def forward(self, encoded_music, tokenized_prompt):\n",
    "        mask = tokenized_prompt['attention_mask']\n",
    "        with self.music_model_conditioner.autocast and torch.set_grad_enabled(True):\n",
    "            x_e = self.text_model(**tokenized_prompt).last_hidden_state\n",
    "        x_e = self.music_model_conditioner.output_proj(x_e.to(self.music_model_conditioner.output_proj.weight))\n",
    "        x_e = (x_e * mask.unsqueeze(-1))\n",
    "        with self.music_model.autocast:\n",
    "            x = self.music_model.lm.compute_predictions(encoded_music, [], {'description': (x_e, mask)})\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        music, prompt = batch['encoded_music'], batch['tokenized_prompt']\n",
    "        out = self(music, prompt)\n",
    "        ce_loss, _ = compute_cross_entropy(out.logits, music, out.mask)\n",
    "        ortho_loss = compute_ortho_loss(self.text_model.shared.weight[batch['batch_tokens']])\n",
    "        loss = self.entropy_alpha * ce_loss + self.ortho_alpha * ortho_loss\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"ortho_loss\", ortho_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        music, prompt = batch['encoded_music'], batch['tokenized_prompt']\n",
    "        out = self(music, prompt)\n",
    "        val_loss, _ = compute_cross_entropy(out.logits, music, out.mask)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Optimizer and learning rate scheduler setup\n",
    "        optimizer = Adam([self.text_model.shared.weight], lr=1e-1)\n",
    "        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return ([optimizer], \n",
    "                []\n",
    "                )\n",
    "class GenEvalCallback(L.Callback):\n",
    "    def __init__(self, generation_concepts, fad, n_epochs=2):\n",
    "        super().__init__()\n",
    "        self.n_epochs = n_epochs\n",
    "        self.concepts = generation_concepts\n",
    "        self.fad = fad\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if (trainer.current_epoch+1) % self.n_epochs == 0:\n",
    "            print(f\"Generation time at epoch {trainer.current_epoch + 1}\")\n",
    "            concept = self.concepts[0]\n",
    "            response = pl_module.music_model.generate([f'In the style of {TokensProvider(5).get_str(concept)}']*3)\n",
    "            for a_idx in range(response.shape[0]):\n",
    "                music = response[a_idx].cpu()\n",
    "                music = music/np.max(np.abs(music.numpy()))\n",
    "                path = OUTPUT_PATH(\"textual-inversion-v2\", concept, 'temp', f'music_p{a_idx}')\n",
    "                audio_write(path, music, pl_module.music_model.cfg.sample_rate)\n",
    "                pl_module.logger.experiment.add_audio(f\"{concept} {a_idx}\", music, trainer.global_step, sample_rate=pl_module.music_model.cfg.sample_rate)\n",
    "            with contextlib.redirect_stdout(io.StringIO()):\n",
    "                fd_score = self.fad.score(INPUT_PATH('textual-inversion-v2', 'data', 'valid', f'{concept}', 'audio'), OUTPUT_PATH(\"textual-inversion-v2\", concept, 'temp'))\n",
    "                os.remove(OUTPUT_PATH(\"textual-inversion-v2\", concept, 'temp_fad_feature_cache.npy'))\n",
    "                pl_module.log('FAD', list(fd_score.values())[0], trainer.global_step)\n",
    "\n",
    "music_model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "music_model.set_generation_params(\n",
    "\tuse_sampling=True,\n",
    "\ttop_k=250,\n",
    "\tduration=EXAMPLES_LEN\n",
    ")\n",
    "text_conditioner=list(music_model.lm.condition_provider.conditioners.values())[0]\n",
    "tokenizer=text_conditioner.t5_tokenizer\n",
    "text_model=text_conditioner.t5\n",
    "fad = FrechetAudioDistance()\n",
    "\n",
    "dm = ConceptDataModule(tokenizer, music_len=255)\n",
    "model = TransformerTextualInversion(text_model, tokenizer, music_model, text_conditioner, lambda: dm.get_new_tokens())\n",
    "tb_logger = L.loggers.TensorBoardLogger(LOGS_PATH, name='textual-inversion-v2')\n",
    "trainer = L.Trainer(accelerator='cpu', callbacks=[GenEvalCallback(['upbeat'], fad)], enable_checkpointing=False, logger=tb_logger)\n",
    "trainer.fit(model, dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen-YATmys4o-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
