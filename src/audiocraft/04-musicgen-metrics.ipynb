{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH, RAW_PATH\n",
    "import torch\n",
    "from audiocraft.data.audio import audio_read, audio_write\n",
    "from audiocraft.data.audio_utils import convert_audio_channels, convert_audio\n",
    "import numpy as np\n",
    "from audioldm_eval.metrics.fad import FrechetAudioDistance\n",
    "import os\n",
    "import sys\n",
    "from fadtk.fad import FrechetAudioDistance, log\n",
    "from fadtk.model_loader import CLAPLaionModel, VGGishModel\n",
    "from fadtk.fad_batch import cache_embedding_files\n",
    "from audiocraft.models import MusicGen\n",
    "import shutil\n",
    "import contextlib\n",
    "import io\n",
    "import warnings\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "EXAMPLES_LEN = 5\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# model = CLAPLaionModel('music')\n",
    "model = VGGishModel()\n",
    "eval_dir = OUTPUT_PATH('concepts-dataset', '8bit-slow', 'temp')\n",
    "cache_embedding_files('fma_pop', model)\n",
    "cache_embedding_files(eval_dir, model)\n",
    "fad = FrechetAudioDistance(model, audio_load_worker=8, load_model=False)\n",
    "fad.score('fma_pop', eval_dir)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "music_model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "music_model.set_generation_params(\n",
    "\tuse_sampling=True,\n",
    "\ttop_k=250,\n",
    "\tduration=EXAMPLES_LEN\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "res=music_model.generate([f\"music in the style of jazz song\"]*5, progress=True)\n",
    "for a_idx in range(res.shape[0]):\n",
    "    music = res[a_idx].cpu()\n",
    "    music = music/np.max(np.abs(music.numpy()))\n",
    "    path = OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp', f'music_p{a_idx}')\n",
    "    audio_write(path, music, music_model.cfg.sample_rate)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cache_embedding_files(OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp'), model)\n",
    "score = fad.score('fma_pop', OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp'))\n",
    "shutil.rmtree(os.path.join(OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp'), 'embeddings'))\n",
    "shutil.rmtree(os.path.join(OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp'), 'convert'))\n",
    "shutil.rmtree(os.path.join(OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp'), 'stats'))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen-ufgTm-Qc-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
