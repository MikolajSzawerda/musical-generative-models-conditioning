{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.util_tools import compute_cross_entropy, compute_ortho_loss\n",
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH\n",
    "\n",
    "import audiocraft\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "import torch\n",
    "from gradio.cli.commands.components.publish import colors\n",
    "from omegaconf import DictConfig\n",
    "from torch import set_grad_enabled\n",
    "from torch.onnx.symbolic_opset9 import tensor\n",
    "from torchviz import make_dot\n",
    "import typing as tp\n",
    "from audiocraft.modules.conditioners import ConditioningAttributes\n",
    "import tqdm\n",
    "import torch\n",
    "from audiocraft.data.audio import audio_read, audio_write\n",
    "from audiocraft.data.audio_utils import convert_audio_channels, convert_audio\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.decomposition import PCA\n",
    "from random import shuffle\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "from audioldm_eval.metrics.fad import FrechetAudioDistance\n",
    "import os\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "EXAMPLES_LEN = 5\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FrechetAudioDistance.__init__() got an unexpected keyword argument 'sampling_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fad \u001b[38;5;241m=\u001b[39m \u001b[43mFrechetAudioDistance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model = MusicGen.get_pretrained('facebook/musicgen-small')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model.set_generation_params(\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# \tuse_sampling=True,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# \ttop_k=250,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# \tduration=EXAMPLES_LEN\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: FrechetAudioDistance.__init__() got an unexpected keyword argument 'sampling_rate'"
     ]
    }
   ],
   "source": [
    "fad = FrechetAudioDistance(verbose=True)\n",
    "# model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "# model.set_generation_params(\n",
    "# \tuse_sampling=True,\n",
    "# \ttop_k=250,\n",
    "# \tduration=EXAMPLES_LEN\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload fad_generated_folder_cache /home/mszawerda/musical-generative-models-conditioning/data/input/textual-inversion/metal_fad_fad_feature_cache.npy\n",
      "Loading data to RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 54.18it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 237.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'frechet_audio_distance': 30.598606179254183}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   253 /    253\r"
     ]
    }
   ],
   "source": [
    "res=model.generate([f\"music in the style of jazz song\"]*5, progress=True)\n",
    "for a_idx in range(res.shape[0]):\n",
    "    music = res[a_idx].cpu()\n",
    "    music = music/np.max(np.abs(music.numpy()))\n",
    "    path = OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp', f'music_p{a_idx}')\n",
    "    audio_write(path, music, model.cfg.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data to RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 28.23it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 158.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data to RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 29.43it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 191.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frechet Audio Distance] exception thrown, Imaginary component 0.25250462336960655\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# fd_score = fad.score(INPUT_PATH('textual-inversion-v3', 'data', 'train', 'cluster_0', 'audio'), OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp'))\n",
    "# fd_score = fad.score(INPUT_PATH('textual-inversion-v3', 'data', 'train', 'cluster_0', 'audio'), OUTPUT_PATH('textual-inversion-v3', 'cluster_10', 'temp'))\n",
    "fd_score = fad.score(OUTPUT_PATH('textual-inversion-v3', 'cluster_2', 'temp'), OUTPUT_PATH('textual-inversion-v3', 'cluster_10', 'temp'), recalculate=True)\n",
    "# fd_score = fad.score(INPUT_PATH('textual-inversion-v3', 'data', 'train', 'cluster_0', 'audio'), INPUT_PATH('textual-inversion-v3', 'data', 'train', 'cluster_1', 'audio'))\n",
    "print(fd_score)\n",
    "# os.remove(OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp_fad_feature_cache.npy'))\n",
    "# list(fd_score.values())[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Audiocraft",
   "language": "python",
   "name": "audiocraft_lab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
