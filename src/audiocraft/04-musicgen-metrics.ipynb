{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH, RAW_PATH\n",
    "import torch\n",
    "from audiocraft.data.audio import audio_read, audio_write\n",
    "from audiocraft.data.audio_utils import convert_audio_channels, convert_audio\n",
    "import numpy as np\n",
    "from audioldm_eval.metrics.fad import FrechetAudioDistance\n",
    "import os\n",
    "import sys\n",
    "from fadtk.fad import FrechetAudioDistance, log, calc_frechet_distance\n",
    "from fadtk.model_loader import CLAPLaionModel, VGGishModel\n",
    "from fadtk.fad_batch import cache_embedding_files\n",
    "from audiocraft.models import MusicGen\n",
    "import shutil\n",
    "import contextlib\n",
    "import io\n",
    "import warnings\n",
    "import torch.multiprocessing as mp\n",
    "from toolz import concat\n",
    "import json\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "sys.path.append(os.path.abspath(\"src\"))\n",
    "from src.callbacks import offline_eval\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "EXAMPLES_LEN = 5\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = CLAPLaionModel('music')\n",
    "# model = VGGishModel()\n",
    "eval_dir = INPUT_PATH('concepts-dataset', 'data', 'train', '8bit', 'audio')\n",
    "# cache_embedding_files('fma_pop', model)\n",
    "# cache_embedding_files(eval_dir, model)\n",
    "fad = FrechetAudioDistance(model, audio_load_worker=8, load_model=True)\n",
    "# fad.score('fma_pop', eval_dir)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "from toolz import partition_all\n",
    "\n",
    "\n",
    "def calc_eval(base_dir: str, concepts: list[str], descriptions: dict[str, str], workers=2):\n",
    "    concepts_batches = list(partition_all(len(concepts) // workers, concepts))\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    with torch.multiprocessing.Pool(processes=workers) as executor:\n",
    "        results = list(\n",
    "            executor.starmap(\n",
    "                offline_eval,\n",
    "                [\n",
    "                    (base_dir, batch, {k: descriptions[k] for k in batch})\n",
    "                    for batch in concepts_batches\n",
    "                ]\n",
    "                # [base_dir] * len(concepts_batches),\n",
    "                # concepts_batches,\n",
    "                # [{k: descriptions[k] for k in batch} for batch in concepts_batches],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return {k: v for val in results for k, v in val.items()}\n",
    "\n",
    "\n",
    "with open(INPUT_PATH('concepts-dataset', \"metadata_concepts.json\"), \"r\") as fh:\n",
    "    concept_descriptions = json.load(fh)\n",
    "calc_eval('concepts-dataset', list(concept_descriptions.keys()), concept_descriptions, workers=4)\n",
    "# offline_eval('concepts-dataset', list(concept_descriptions.keys()), concept_descriptions)\n",
    "# offline_eval('textual-inversion-v3', ['ichika', 'caravan', 'metal', 'ajfa'], {'ichika': 'aa', 'caravan': 'bb', 'metal': 'aa', 'ajfa': 'bb'})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "mu_bg, cov_bg = fad.load_stats('fma_pop')\n",
    "mu_eval, cov_eval = fad.load_stats(eval_dir)\n",
    "\n",
    "calc_frechet_distance(mu_bg, cov_bg, mu_eval, cov_eval)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "audio_embeds = fad.load_embeddings(eval_dir)\n",
    "text_embeds = model.model.get_text_embedding(\n",
    "    \"Guitar backing track in the rock genre, played in B minor, often used for improvisation and jamming.\").reshape(-1)\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "np.mean(cosine_similarity(audio_embeds, text_embeds))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "music_model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "music_model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=EXAMPLES_LEN\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "res = music_model.generate([f\"music in the style of jazz song\"] * 5, progress=True)\n",
    "for a_idx in range(res.shape[0]):\n",
    "    music = res[a_idx].cpu()\n",
    "    music = music / np.max(np.abs(music.numpy()))\n",
    "    path = OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp', f'music_p{a_idx}')\n",
    "    audio_write(path, music, music_model.cfg.sample_rate)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cache_embedding_files(OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp'), model)\n",
    "score = fad.score('fma_pop', OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp'))\n",
    "shutil.rmtree(os.path.join(OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp'), 'embeddings'))\n",
    "shutil.rmtree(os.path.join(OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp'), 'convert'))\n",
    "shutil.rmtree(os.path.join(OUTPUT_PATH(\"textual-inversion\", 'metal', 'temp'), 'stats'))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen-ufgTm-Qc-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
