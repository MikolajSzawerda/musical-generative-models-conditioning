{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:52:38.644076Z",
     "start_time": "2024-10-16T07:52:32.791197Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.util_tools import compute_cross_entropy, compute_ortho_loss\n",
    "from tools.project import INPUT_PATH, LOGS_PATH, OUTPUT_PATH\n",
    "\n",
    "import audiocraft\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "import torch\n",
    "from gradio.cli.commands.components.publish import colors\n",
    "from omegaconf import DictConfig\n",
    "from torch import set_grad_enabled\n",
    "from torch.onnx.symbolic_opset9 import tensor\n",
    "from torchviz import make_dot\n",
    "import typing as tp\n",
    "from audiocraft.modules.conditioners import ConditioningAttributes\n",
    "import tqdm\n",
    "import torch\n",
    "from audiocraft.data.audio import audio_read, audio_write\n",
    "from audiocraft.data.audio_utils import convert_audio_channels, convert_audio\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.decomposition import PCA\n",
    "from random import shuffle\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "def count_directories(path):\n",
    "    import os\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    return sum(os.path.isdir(os.path.join(path, entry)) for entry in os.listdir(path))\n",
    "\n",
    "EXP_NUM = count_directories(LOGS_PATH('textual-inversion'))+1\n",
    "EXAMPLES_LEN = 5\n",
    "BATCH_SIZE = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd6886027f90f11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:52:44.198714Z",
     "start_time": "2024-10-16T07:52:38.706105Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mszawerda/.cache/pypoetry/virtualenvs/musicgen-ufgTm-Qc-py3.10/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "model.set_generation_params(\n",
    "\tuse_sampling=True,\n",
    "\ttop_k=250,\n",
    "\tduration=EXAMPLES_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63c34929631fd62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T08:30:46.460477Z",
     "start_time": "2024-10-16T08:30:13.388886Z"
    }
   },
   "outputs": [],
   "source": [
    "EXP_NAME = 'organs'\n",
    "def get_music(idxs):\n",
    "\tsongs = []\n",
    "\tfor p in idxs:\n",
    "\t\tmusic, sr = audio_read(INPUT_PATH('textual-inversion', EXP_NAME, f'music_p{p}.wav'), duration=EXAMPLES_LEN, pad=True)\n",
    "\t\tmusic = music[None]\n",
    "\t\tsongs.append(convert_audio(music, sr, 32000, 1))\n",
    "\twith torch.no_grad():\n",
    "\t\tencoded_music, _ = model.compression_model.encode(torch.concatenate(songs).to(DEVICE))\n",
    "\treturn encoded_music\n",
    "def load_music_to_pt():\n",
    "\treturn torch.concatenate([get_music(range(i, i+10)) for i in range(0, 100, 10)])\n",
    "# torch.save(load_music_to_pt(), INPUT_PATH('textual-inversion', EXP_NAME, 'encoded.pt'))\n",
    "\n",
    "dl = lambda x, s: DataLoader(x, batch_size=BATCH_SIZE, shuffle=s, pin_memory=True if torch.cuda.is_available() else False)\n",
    "ds=torch.load(INPUT_PATH('textual-inversion', EXP_NAME, 'encoded.pt'))[:225, :, :].cpu()\n",
    "ds = TensorDataset(ds)\n",
    "train_ds, val_ds = random_split(ds, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "train_dl, val_dl = dl(train_ds, True), dl(val_ds, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34969ddc66df4904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T07:52:47.870937Z",
     "start_time": "2024-10-16T07:52:47.583558Z"
    }
   },
   "outputs": [],
   "source": [
    "letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "N_TOKENS = 10\n",
    "new_tokens = [f\"<{l}*>\" for l in letters[:N_TOKENS]]\n",
    "text_conditioner=list(model.lm.condition_provider.conditioners.values())[0]\n",
    "tokenizer=text_conditioner.t5_tokenizer\n",
    "text_model=text_conditioner.t5.to(DEVICE)\n",
    "\n",
    "if tokenizer.add_tokens(new_tokens) > 0:\n",
    "\ttext_model.resize_token_embeddings(len(tokenizer))\n",
    "\t\n",
    "new_token_ids = tokenizer.convert_tokens_to_ids(new_tokens)\n",
    "with torch.no_grad():\n",
    "\tfor new_token_id in new_token_ids:\n",
    "\t\ttext_model.shared.weight[new_token_id] = text_model.shared.weight.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f77a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens_str = \" \".join(new_tokens)\n",
    "train_desc = [\n",
    "    f\"the sound of {new_tokens_str}\",\n",
    "    f\"pure {new_tokens_str} audio\",\n",
    "    f\"the recorded {new_tokens_str} sound\",\n",
    "    f\"{new_tokens_str} audio sample\",\n",
    "    f\"recording of {new_tokens_str}\",\n",
    "    f\"high fidelity {new_tokens_str} audio\",\n",
    "    f\"{new_tokens_str} sound clip\",\n",
    "    f\"audio of {new_tokens_str}\",\n",
    "    f\"captured {new_tokens_str} sound\",\n",
    "    f\"{new_tokens_str} audio recording\"\n",
    "][:BATCH_SIZE]\n",
    "\n",
    "val_desc = [\n",
    "    f\"audio capture of {new_tokens_str}\",\n",
    "    f\"{new_tokens_str} sound recording\",\n",
    "    f\"pristine {new_tokens_str} audio\",\n",
    "    f\"clear {new_tokens_str} recording\",\n",
    "    f\"the audio of {new_tokens_str}\",\n",
    "    f\"{new_tokens_str} audio sample capture\",\n",
    "    f\"the recorded sound of {new_tokens_str}\",\n",
    "    f\"sample of {new_tokens_str} audio\",\n",
    "    f\"{new_tokens_str} audio segment\",\n",
    "    f\"recorded audio of {new_tokens_str}\"\n",
    "][:BATCH_SIZE]\n",
    "\n",
    "val_desc = [\n",
    "    ConditioningAttributes(text={'description': x})\n",
    "    for x in val_desc\n",
    "]\n",
    "\n",
    "assert len(train_desc)==BATCH_SIZE and len(val_desc)==BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7dbb61b7258644a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T08:58:32.710898Z",
     "start_time": "2024-10-16T08:31:05.186587Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 8/16:   9%|â–‰         | 9/100 [00:18<03:03,  2.01s/it, epoch: 8, grad_norm: 0.124, loss: 3.995] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mautocast:\n\u001b[1;32m     49\u001b[0m \tout \u001b[38;5;241m=\u001b[39m lm\u001b[38;5;241m.\u001b[39mcompute_predictions(music_batch, [], {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m: (embeds, mask)})\n\u001b[0;32m---> 50\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmusic_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m ortho_loss \u001b[38;5;241m=\u001b[39m compute_ortho_loss(text_model\u001b[38;5;241m.\u001b[39mshared\u001b[38;5;241m.\u001b[39mweight[new_token_ids])\n\u001b[1;32m     52\u001b[0m final_loss \u001b[38;5;241m=\u001b[39m ce_l\u001b[38;5;241m*\u001b[39mloss \u001b[38;5;241m+\u001b[39m ortho_l \u001b[38;5;241m*\u001b[39m ortho_loss\n",
      "File \u001b[0;32m~/musical-generative-models-conditioning/src/audiocraft/src/util_tools.py:30\u001b[0m, in \u001b[0;36mcompute_cross_entropy\u001b[0;34m(logits, targets, mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m ce_targets \u001b[38;5;241m=\u001b[39m targets_k[mask_k]\n\u001b[1;32m     29\u001b[0m ce_logits \u001b[38;5;241m=\u001b[39m logits_k[mask_k]\n\u001b[0;32m---> 30\u001b[0m q_ce \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mce_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mce_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m ce \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m q_ce\n\u001b[1;32m     32\u001b[0m ce_per_codebook\u001b[38;5;241m.\u001b[39mappend(q_ce\u001b[38;5;241m.\u001b[39mdetach())\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/musicgen-ufgTm-Qc-py3.10/lib/python3.10/site-packages/torch/nn/functional.py:2963\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2958\u001b[0m         reduced \u001b[38;5;241m=\u001b[39m reduced \u001b[38;5;241m/\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2960\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduced\n\u001b[0;32m-> 2963\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_entropy\u001b[39m(\n\u001b[1;32m   2964\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m   2965\u001b[0m     target: Tensor,\n\u001b[1;32m   2966\u001b[0m     weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2967\u001b[0m     size_average: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2968\u001b[0m     ignore_index: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m   2969\u001b[0m     reduce: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2970\u001b[0m     reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2971\u001b[0m     label_smoothing: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   2972\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m   2973\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"This criterion computes the cross entropy loss between input logits and target.\u001b[39;00m\n\u001b[1;32m   2974\u001b[0m \n\u001b[1;32m   2975\u001b[0m \u001b[38;5;124;03m    See :class:`~torch.nn.CrossEntropyLoss` for details.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;124;03m        >>> loss.backward()\u001b[39;00m\n\u001b[1;32m   3037\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target, weight):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lm = model.lm\n",
    "lm.requires_grad_(True)\n",
    "text_model.requires_grad_(True)\n",
    "text_conditioner.finetune=True\n",
    "\n",
    "new_token_ids = tokenizer.convert_tokens_to_ids(new_tokens)\n",
    "def zero_existing_emb(grad):\n",
    "\tmask = torch.zeros_like(grad)\n",
    "\tfor new_token_id in new_token_ids:\n",
    "\t\tmask[new_token_id] = 10.0\n",
    "\treturn grad * mask\n",
    "\n",
    "text_model.shared.weight.register_hook(zero_existing_emb)\n",
    "\n",
    "target = text_model.shared.weight\n",
    "# old_target = target[new_token_id].detach().clone()\n",
    "writer = SummaryWriter(LOGS_PATH('textual-inversion', f'{EXP_NUM}'))\n",
    "EXP_NUM += 1\n",
    "val_step = True\n",
    "\n",
    "text_conditioner = list(lm.condition_provider.conditioners.values())[0]\n",
    "lr = 1e-1\n",
    "ortho_l = 1e-2\n",
    "ce_l = 1e1\n",
    "optimizer = torch.optim.Adam([target], lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "# target_path = []\n",
    "\n",
    "tokenized_desc = {\n",
    "\tk: v.to(DEVICE)\n",
    "\tfor k,v in tokenizer(train_desc, return_tensors='pt', padding=True, add_special_tokens=False).items()\n",
    "}\n",
    "mask = tokenized_desc['attention_mask'].to(DEVICE)\n",
    "\n",
    "with tqdm.tqdm(total=epochs) as pbar:\n",
    "\tfor epoch in range(epochs):\n",
    "\t\t# target_path.append(target[new_token_id].detach().clone())\n",
    "\t\ttotal_loss = 0\n",
    "\t\ttotal_ortho_loss = 0\n",
    "\t\tnum_batches = len(train_dl)\n",
    "\t\tfor i, music_batch in enumerate(train_dl):\n",
    "\t\t\tmusic_batch = music_batch[0].to(DEVICE)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\twith text_conditioner.autocast and torch.set_grad_enabled(True):\n",
    "\t\t\t\tembeds = text_model(**tokenized_desc).last_hidden_state\n",
    "\t\t\tembeds = text_conditioner.output_proj(embeds.to(text_conditioner.output_proj.weight))\n",
    "\t\t\tembeds = (embeds * mask.unsqueeze(-1))\n",
    "\t\t\twith model.autocast:\n",
    "\t\t\t\tout = lm.compute_predictions(music_batch, [], {'description': (embeds, mask)})\n",
    "\t\t\tloss, _ = compute_cross_entropy(out.logits, music_batch, out.mask)\n",
    "\t\t\tortho_loss = compute_ortho_loss(text_model.shared.weight[new_token_ids])\n",
    "\t\t\tfinal_loss = ce_l*loss + ortho_l * ortho_loss\n",
    "\t\t\tfinal_loss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttotal_loss += final_loss.item()\n",
    "\t\t\ttotal_ortho_loss += ortho_loss.item()\n",
    "\t\t\tpbar.set_description(f'train {i}/{num_batches}')\n",
    "\t\twriter.add_scalar('avg loss', total_loss/num_batches, epoch)\n",
    "\t\twriter.add_scalar('avg ortho loss', total_ortho_loss/num_batches, epoch)\n",
    "\t\t# writer.add_scalar('grad norm', target[new_token_id].grad.norm().item(), i)\n",
    "\t\t\n",
    "\t\tif epoch % 10==0 and val_step:\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\ttotal_loss = 0\n",
    "\t\t\t\tnum_batches = len(val_dl)\n",
    "\t\t\t\tfor music_batch in val_dl:\n",
    "\t\t\t\t\tmusic_batch = music_batch[0].to(DEVICE)\n",
    "\t\t\t\t\twith model.autocast:\n",
    "\t\t\t\t\t\tval_out = lm.compute_predictions(music_batch, val_desc)\n",
    "\t\t\t\t\tval_loss, _ = compute_cross_entropy(val_out.logits, music_batch, val_out.mask)\n",
    "\t\t\t\t\tscheduler.step(val_loss)\n",
    "\n",
    "\t\t\t\t\ttotal_loss += val_loss\n",
    "\t\t\t\t\tpbar.set_description(f'val {i}/{num_batches}')\n",
    "\t\t\t\twriter.add_scalar('avg val loss', total_loss/num_batches, epoch)\n",
    "\t\t\t\tres=model.generate([\"In the style of \"+new_tokens_str]*5)\n",
    "\t\t\t\tfor a_idx in range(res.shape[0]):\n",
    "\t\t\t\t\tmusic = res[a_idx].cpu()\n",
    "\t\t\t\t\twriter.add_audio(f'Audio {a_idx}', music/np.max(np.abs(music.numpy())), epoch, model.cfg.sample_rate)\n",
    "\t\tpbar.set_postfix_str(f'epoch: {epoch}, grad_norm: {target.grad.norm().item():.3f}, loss: {loss.item():.3f}')\n",
    "\t\tpbar.update(1)\n",
    "\t# target_path.append(target[new_token_id].detach().clone())\n",
    "writer.close()\n",
    "# torch.norm(target[new_token_id].detach().clone()-old_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842c2925c7394c6",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a37f9f6e7aa349",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T08:59:56.987693Z",
     "start_time": "2024-10-16T08:59:21.207713Z"
    }
   },
   "outputs": [],
   "source": [
    "# res=model.generate([f\"music with {new_token}\"]*5, progress=True)\n",
    "# display_audio(res,model.cfg.sample_rate )\n",
    "# # for i, audio in enumerate(res):\n",
    "# # \taudio_write(OUTPUT_PATH('textual-inversion', f'out_{i}'), audio, model.cfg.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19903524bc426667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T09:00:42.764406Z",
     "start_time": "2024-10-16T09:00:42.762238Z"
    }
   },
   "outputs": [],
   "source": [
    "# sentences = [\n",
    "# \t\"Dog\",\n",
    "# \t\"S*\",\n",
    "# \t\"bass and guitar\",\n",
    "# \t\"post rock\",\n",
    "# \t\"8bit\",\n",
    "# \t\"games\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1816b62e21612f99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T09:00:45.523771Z",
     "start_time": "2024-10-16T09:00:44.594227Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Tokenize sentences\n",
    "# inputs = tokenizer(sentences, return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "\n",
    "# # Get encoder outputs\n",
    "# with torch.no_grad():\n",
    "# \toutputs= text_model.encoder.embed_tokens(inputs['input_ids'].to(DEVICE))\n",
    "# # outputs = text_model(**inputs)\n",
    "\n",
    "\n",
    "# # Compute attention mask\n",
    "# attention_mask = inputs['attention_mask']\n",
    "# attention_mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.size()).float()\n",
    "\n",
    "# # Average pooling\n",
    "# sum_embeddings = torch.sum(outputs * attention_mask_expanded.to(DEVICE), dim=1)\n",
    "# sum_mask = torch.clamp(attention_mask_expanded.sum(dim=1), min=1e-9).to(DEVICE)\n",
    "# sentence_embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "# # Convert to NumPy\n",
    "# sentence_embeddings = sentence_embeddings.cpu().numpy()\n",
    "\n",
    "# # Dimensionality reduction\n",
    "# dim_input = np.concatenate([np.stack([x.cpu() for x in target_path]), sentence_embeddings])\n",
    "# # reducer = umap.UMAP(n_neighbors=5, n_components=2, metric='cosine')\n",
    "# # embeddings_2d = reducer.fit_transform(dim_input)\n",
    "# pca = PCA(n_components=2)\n",
    "# embeddings_2d = pca.fit_transform(dim_input)\n",
    "\n",
    "# # Plotting\n",
    "# sentence_idx = len(target_path)\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.scatter(embeddings_2d[sentence_idx:, 0], embeddings_2d[sentence_idx:, 1], c='blue')\n",
    "\n",
    "# for i, sentence in enumerate(sentences):\n",
    "# \tplt.annotate(sentence, (embeddings_2d[i+sentence_idx, 0], embeddings_2d[i+sentence_idx, 1]))\n",
    "\n",
    "# colors = cm.viridis(np.linspace(0, 1, sentence_idx))\n",
    "# colors[:, 3]=colors[:, 3]-0.5\n",
    "\n",
    "\n",
    "# for (x,y), c in zip(embeddings_2d[:sentence_idx], colors):\n",
    "# \tplt.scatter(x, y, color=c)\n",
    "# plt.plot([x[0] for x in embeddings_2d[:sentence_idx]], [x[1] for x in embeddings_2d[:sentence_idx]], ':g', alpha=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# plt.title('Sentence Embeddings Visualized with UMAP (T5)')\n",
    "# plt.xlabel('Dimension 1')\n",
    "# plt.ylabel('Dimension 2')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a9a51cee6c6e79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen-ufgTm-Qc-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
