[
	{
		"id": "http://zotero.org/users/14181977/items/MGQ7YTIL",
		"type": "book"
	},
	{
		"id": "http://zotero.org/users/14181977/items/V3DJQWRS",
		"type": "webpage",
		"title": "Zotero | Email validation",
		"URL": "https://www.zotero.org/user/validate/77052e655228ead844f6",
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/BEML3DTX",
		"type": "article",
		"abstract": "We introduce Self-Monitored Inference-Time INtervention (SMITIN), an approach for controlling an autoregressive generative music transformer using classifier probes. These simple logistic regression probes are trained on the output of each attention head in the transformer using a small dataset of audio examples both exhibiting and missing a specific musical trait (e.g., the presence/absence of drums, or real/synthetic music). We then steer the attention heads in the probe direction, ensuring the generative model output captures the desired musical trait. Additionally, we monitor the probe output to avoid adding an excessive amount of intervention into the autoregressive generation, which could lead to temporally incoherent music. We validate our results objectively and subjectively for both audio continuation and text-to-music applications, demonstrating the ability to add controls to large generative models for which retraining or even fine-tuning is impractical for most musicians.",
		"language": "en",
		"note": "arXiv:2404.02252 [cs, eess]",
		"number": "arXiv:2404.02252",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "SMITIN: Self-Monitored Inference-Time INtervention for Generative Music Transformers",
		"title-short": "SMITIN",
		"URL": "http://arxiv.org/abs/2404.02252",
		"author": [
			{
				"family": "Koo",
				"given": "Junghyun"
			},
			{
				"family": "Wichern",
				"given": "Gordon"
			},
			{
				"family": "Germain",
				"given": "Francois G."
			},
			{
				"family": "Khurana",
				"given": "Sameer"
			},
			{
				"family": "Roux",
				"given": "Jonathan Le"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					2
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/P2PV74R4",
		"type": "article",
		"abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.",
		"language": "en",
		"note": "arXiv:2109.03264 [cs, eess]",
		"number": "arXiv:2109.03264",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Text-Free Prosody-Aware Generative Spoken Language Modeling",
		"URL": "http://arxiv.org/abs/2109.03264",
		"author": [
			{
				"family": "Kharitonov",
				"given": "Eugene"
			},
			{
				"family": "Lee",
				"given": "Ann"
			},
			{
				"family": "Polyak",
				"given": "Adam"
			},
			{
				"family": "Adi",
				"given": "Yossi"
			},
			{
				"family": "Copet",
				"given": "Jade"
			},
			{
				"family": "Lakhotia",
				"given": "Kushal"
			},
			{
				"family": "Nguyen",
				"given": "Tu-Anh"
			},
			{
				"family": "Rivière",
				"given": "Morgane"
			},
			{
				"family": "Mohamed",
				"given": "Abdelrahman"
			},
			{
				"family": "Dupoux",
				"given": "Emmanuel"
			},
			{
				"family": "Hsu",
				"given": "Wei-Ning"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					10
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/BZ57CHIZ",
		"type": "article",
		"abstract": "Transformer-based encoder-decoder models produce a fused token-wise representation after every encoder layer. We investigate the effects of allowing the encoder to preserve and explore alternative hypotheses, combined at the end of the encoding process. To that end, we design and examine a Multi-stream Transformer architecture and ﬁnd that splitting the Transformer encoder into multiple encoder streams and allowing the model to merge multiple representational hypotheses improves performance, with further improvement obtained by adding a skip connection between the ﬁrst and the ﬁnal encoder layer.",
		"language": "en",
		"note": "arXiv:2107.10342 [cs]",
		"number": "arXiv:2107.10342",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Multi-Stream Transformers",
		"URL": "http://arxiv.org/abs/2107.10342",
		"author": [
			{
				"family": "Burtsev",
				"given": "Mikhail"
			},
			{
				"family": "Rumshisky",
				"given": "Anna"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					21
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/IXHGNV4G",
		"type": "article",
		"abstract": "We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AUDIOGEN, an auto-regressive generative model that generates audio samples conditioned on text inputs. AUDIOGEN operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating “objects” can be a difﬁcult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-ﬁdelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classiﬁer-free guidance to improve adherence to text. Comparing to the evaluated baselines, AUDIOGEN outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: https://felixkreuk.github.io/audiogen.",
		"language": "en",
		"note": "arXiv:2209.15352 [cs, eess]",
		"number": "arXiv:2209.15352",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AudioGen: Textually Guided Audio Generation",
		"title-short": "AudioGen",
		"URL": "http://arxiv.org/abs/2209.15352",
		"author": [
			{
				"family": "Kreuk",
				"given": "Felix"
			},
			{
				"family": "Synnaeve",
				"given": "Gabriel"
			},
			{
				"family": "Polyak",
				"given": "Adam"
			},
			{
				"family": "Singer",
				"given": "Uriel"
			},
			{
				"family": "Défossez",
				"given": "Alexandre"
			},
			{
				"family": "Copet",
				"given": "Jade"
			},
			{
				"family": "Parikh",
				"given": "Devi"
			},
			{
				"family": "Taigman",
				"given": "Yaniv"
			},
			{
				"family": "Adi",
				"given": "Yossi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					5
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/VLYXMJQG",
		"type": "article",
		"abstract": "We tackle the task of conditional music generation. We introduce MUSICGEN, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MUSICGEN is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MUSICGEN can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MUSICGEN. Music samples, code, and models are available at github.com/facebookresearch/audiocraft.",
		"language": "en",
		"note": "arXiv:2306.05284 [cs, eess]",
		"number": "arXiv:2306.05284",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Simple and Controllable Music Generation",
		"URL": "http://arxiv.org/abs/2306.05284",
		"author": [
			{
				"family": "Copet",
				"given": "Jade"
			},
			{
				"family": "Kreuk",
				"given": "Felix"
			},
			{
				"family": "Gat",
				"given": "Itai"
			},
			{
				"family": "Remez",
				"given": "Tal"
			},
			{
				"family": "Kant",
				"given": "David"
			},
			{
				"family": "Synnaeve",
				"given": "Gabriel"
			},
			{
				"family": "Adi",
				"given": "Yossi"
			},
			{
				"family": "Défossez",
				"given": "Alexandre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					1,
					29
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/WNENKEJ8",
		"type": "article",
		"abstract": "Music generation has generally been focused on either creating scores or interpreting them. We discuss diﬀerences between these two problems and propose that, in fact, it may be valuable to work in the space of direct performance generation: jointly predicting the notes and also their expressive timing and dynamics. We consider the signiﬁcance and qualities of the data set needed for this. Having identiﬁed both a problem domain and characteristics of an appropriate data set, we show an LSTM-based recurrent network model that subjectively performs quite well on this task. Critically, we provide generated examples. We also include feedback from professional composers and musicians about some of these examples.",
		"language": "en",
		"note": "arXiv:1808.03715 [cs, eess]",
		"number": "arXiv:1808.03715",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "This Time with Feeling: Learning Expressive Musical Performance",
		"title-short": "This Time with Feeling",
		"URL": "http://arxiv.org/abs/1808.03715",
		"author": [
			{
				"family": "Oore",
				"given": "Sageev"
			},
			{
				"family": "Simon",
				"given": "Ian"
			},
			{
				"family": "Dieleman",
				"given": "Sander"
			},
			{
				"family": "Eck",
				"given": "Douglas"
			},
			{
				"family": "Simonyan",
				"given": "Karen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					8,
					10
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/HXSZWZZL",
		"type": "post-weblog",
		"abstract": "Generating MIDI with LSTMs and an additional conditioning vector",
		"container-title": "Medium",
		"language": "en",
		"title": "PerformanceRNN with Note-On Conditioning",
		"URL": "https://robzz.medium.com/performancernn-with-note-on-conditioning-fac981f82d10",
		"author": [
			{
				"family": "robz",
				"given": ""
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12,
					27
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/CQJEZ3VA",
		"type": "article",
		"abstract": "We explore the use of large language models (LLMs) for music generation using a retrieval system to select relevant examples. We find promising initial results for music generation in a dialogue with the user, especially considering the ease with which such a system can be implemented. The code is available online.",
		"language": "en",
		"note": "arXiv:2311.10384 [cs, eess]",
		"number": "arXiv:2311.10384",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Retrieval Augmented Generation of Symbolic Music with LLMs",
		"URL": "http://arxiv.org/abs/2311.10384",
		"author": [
			{
				"family": "Jonason",
				"given": "Nicolas"
			},
			{
				"family": "Casini",
				"given": "Luca"
			},
			{
				"family": "Thomé",
				"given": "Carl"
			},
			{
				"family": "Sturm",
				"given": "Bob L. T."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					28
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/C59F5LY3",
		"type": "post-weblog",
		"abstract": "What is Jaccard Similarity?",
		"container-title": "Medium",
		"language": "en",
		"title": "Jaccard Similarity Made Simple: A Beginner’s Guide to Data Comparison",
		"title-short": "Jaccard Similarity Made Simple",
		"URL": "https://medium.com/@mayurdhvajsinhjadeja/jaccard-similarity-34e2c15fb524",
		"author": [
			{
				"family": "Jadeja",
				"given": "Mayurdhvajsinh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					3,
					5
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/LLZXSKD4",
		"type": "article",
		"abstract": "Performance RNN is a machine-learning system designed primarily for the generation of solo piano performances using an event-based (rather than audio) representation. More speciﬁcally, Performance RNN is a long short-term memory (LSTM) based recurrent neural network that models polyphonic music with expressive timing and dynamics (Oore et al., 2018). The neural network uses a simple language model based on the Musical Instrument Digital Interface (MIDI) ﬁle format. Performance RNN is trained on the e-Piano Junior Competition Dataset (International Piano e-Competition, 2018), a collection of solo piano performances by expert pianists. As an artistic tool, one of the limitations of the original model has been the lack of useable controls. The standard form of Performance RNN can generate interesting pieces, but little control is provided over what speciﬁcally is generated. This paper explores a set of conditioning-based controls used to inﬂuence the generation process.",
		"language": "en",
		"note": "arXiv:1907.04352 [cs, eess]",
		"number": "arXiv:1907.04352",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Exploring Conditioning for Generative Music Systems with Human-Interpretable Controls",
		"URL": "http://arxiv.org/abs/1907.04352",
		"author": [
			{
				"family": "Meade",
				"given": "Nicholas"
			},
			{
				"family": "Barreyre",
				"given": "Nicholas"
			},
			{
				"family": "Lowe",
				"given": "Scott C."
			},
			{
				"family": "Oore",
				"given": "Sageev"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					8,
					3
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/MSGDRM2F",
		"type": "webpage",
		"abstract": "NeurIPS 2023 Workshop",
		"container-title": "Machine Learning for Creativity and Design",
		"language": "en-US",
		"title": "Machine Learning for Creativity and Design",
		"URL": "https://neuripscreativityworkshop.github.io/2023/",
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/KZNWHXMV",
		"type": "webpage",
		"title": "Magenta demos",
		"URL": "https://magenta.tensorflow.org/demos/web/",
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/4KS39AM2",
		"type": "webpage",
		"abstract": "Previously, we introduced Music Transformer, an autoregressive model capable of generating expressive piano performances with long-term structure. We are now...",
		"container-title": "Magenta",
		"language": "en",
		"title": "Generating Piano Music with Transformer",
		"URL": "https://magenta.tensorflow.org/piano-transformer",
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					9,
					16
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/NL9VT8MC",
		"type": "article",
		"abstract": "Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modiﬁed relative attention mechanism can generate minutelong compositions (thousands of steps, four times the length modeled in Oore et al. (2018)) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies1. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.",
		"language": "en",
		"note": "arXiv:1809.04281 [cs, eess, stat]",
		"number": "arXiv:1809.04281",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Music Transformer",
		"URL": "http://arxiv.org/abs/1809.04281",
		"author": [
			{
				"family": "Huang",
				"given": "Cheng-Zhi Anna"
			},
			{
				"family": "Vaswani",
				"given": "Ashish"
			},
			{
				"family": "Uszkoreit",
				"given": "Jakob"
			},
			{
				"family": "Shazeer",
				"given": "Noam"
			},
			{
				"family": "Simon",
				"given": "Ian"
			},
			{
				"family": "Hawthorne",
				"given": "Curtis"
			},
			{
				"family": "Dai",
				"given": "Andrew M."
			},
			{
				"family": "Hoffman",
				"given": "Matthew D."
			},
			{
				"family": "Dinculescu",
				"given": "Monica"
			},
			{
				"family": "Eck",
				"given": "Douglas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					12,
					12
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/PR2HDMPH",
		"type": "article",
		"abstract": "Existing automatic music generation approaches that feature deep learning can be broadly classiﬁed into two types: raw audio models and symbolic models. Symbolic models, which train and generate at the note level, are currently the more prevalent approach; these models can capture long-range dependencies of melodic structure, but fail to grasp the nuances and richness of raw audio generations. Raw audio models, such as DeepMind’s WaveNet, train directly on sampled audio waveforms, allowing them to produce realistic-sounding, albeit unstructured music. In this paper, we propose an automatic music generation methodology combining both of these approaches to create structured, realistic-sounding compositions. We consider a Long Short Term Memory network to learn the melodic structure of different styles of music, and then use the unique symbolic generations from this model as a conditioning input to a WaveNet-based raw audio generator, creating a model for automatic, novel music. We then evaluate this approach by showcasing results of this work.",
		"language": "en",
		"note": "arXiv:1806.09905 [cs, eess, stat]",
		"number": "arXiv:1806.09905",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Conditioning Deep Generative Raw Audio Models for Structured Automatic Music",
		"URL": "http://arxiv.org/abs/1806.09905",
		"author": [
			{
				"family": "Manzelli",
				"given": "Rachel"
			},
			{
				"family": "Thakkar",
				"given": "Vijay"
			},
			{
				"family": "Siahkamari",
				"given": "Ali"
			},
			{
				"family": "Kulis",
				"given": "Brian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					6,
					26
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/VFHI7KYD",
		"type": "article-journal",
		"abstract": "We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-ﬁdelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code.",
		"language": "en",
		"source": "Zotero",
		"title": "Jukebox: A Generative Model for Music",
		"author": [
			{
				"family": "Dhariwal",
				"given": "Prafulla"
			},
			{
				"family": "Jun",
				"given": "Heewoo"
			},
			{
				"family": "Payne",
				"given": "Christine"
			},
			{
				"family": "Kim",
				"given": "Jong Wook"
			},
			{
				"family": "Radford",
				"given": "Alec"
			},
			{
				"family": "Sutskever",
				"given": "Ilya"
			}
		]
	},
	{
		"id": "http://zotero.org/users/14181977/items/ESU4KCHB",
		"type": "article",
		"abstract": "Music is an expressive form of communication often used to convey emotion in scenarios where “words are not enough”. Part of this information lies in the musical composition where well-deﬁned language exists. However, a signiﬁcant amount of information is added during a performance as the musician interprets the composition. The performer injects expressiveness into the written score through variations of different musical properties such as dynamics and tempo. In this paper, we describe a model that can learn to perform sheet music. Our research concludes that the generated performances are indistinguishable from a human performance, thereby passing a test in the spirit of a “musical Turing test”.",
		"language": "en",
		"note": "arXiv:1708.03535 [cs]",
		"number": "arXiv:1708.03535",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Neural Translation of Musical Style",
		"URL": "http://arxiv.org/abs/1708.03535",
		"author": [
			{
				"family": "Malik",
				"given": "Iman"
			},
			{
				"family": "Ek",
				"given": "Carl Henrik"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					8,
					11
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/WIU755JQ",
		"type": "article-journal",
		"abstract": "Artificial intelligence, particularly machine learning, has begun to permeate various real-world applications and is continually being explored in automatic music generation. The approaches to music generation can be broadly divided into two categories: rule-based and data-driven methods. Rule-based approaches rely on substantial prior knowledge and may struggle to handle large datasets, whereas data-driven approaches can solve these problems and have become increasingly popular. However, data-driven approaches still face challenges such as the difficulty of considering long-distance dependencies when handling discrete-sequence data and convergence during model training. Although the diffusion model has been introduced as a generative model to solve the convergence problem in generative adversarial networks, it has not yet been applied to discrete-sequence data. This paper proposes a transformer-based diffusion model known as MelodyDiffusion to handle discrete musical data and realize chord-conditioned melody generation. MelodyDiffusion replaces the U-nets used in traditional diffusion models with transformers to consider the long-distance dependencies using attention and parallel mechanisms. Moreover, a transformer-based encoder is designed to extract contextual information from chords as a condition to guide melody generation. MelodyDiffusion can automatically generate diverse melodies based on the provided chords in practical applications. The evaluation experiments, in which Hits@k was used as a metric to evaluate the restored melodies, demonstrate that the large-scale version of MelodyDiffusion achieves an accuracy of 72.41% (k = 1).",
		"container-title": "Mathematics",
		"DOI": "10.3390/math11081915",
		"ISSN": "2227-7390",
		"issue": "8",
		"language": "en",
		"license": "http://creativecommons.org/licenses/by/3.0/",
		"note": "number: 8\npublisher: Multidisciplinary Digital Publishing Institute",
		"page": "1915",
		"source": "www.mdpi.com",
		"title": "MelodyDiffusion: Chord-Conditioned Melody Generation Using a Transformer-Based Diffusion Model",
		"title-short": "MelodyDiffusion",
		"URL": "https://www.mdpi.com/2227-7390/11/8/1915",
		"volume": "11",
		"author": [
			{
				"family": "Li",
				"given": "Shuyu"
			},
			{
				"family": "Sung",
				"given": "Yunsick"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/ABMXA6XT",
		"type": "article-journal",
		"abstract": "Recent progress in music generation has been remarkably advanced by the stateof-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and ﬁne acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the ﬁne-grained acoustic tokens, making it computationally expensive and prohibitive for a realtime generation. Efﬁcient music generation with a quality on par with MusicLM remains a signiﬁcant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% to 99.6% forward passes in MusicLM, respectively, for sampling 10s to 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efﬁciently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and ﬁne acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and inﬁnitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.",
		"language": "en",
		"source": "Zotero",
		"title": "Efﬁcient Neural Music Generation",
		"author": [
			{
				"family": "Lam",
				"given": "Max W Y"
			},
			{
				"family": "Tian",
				"given": "Qiao"
			},
			{
				"family": "Li",
				"given": "Tang"
			},
			{
				"family": "Yin",
				"given": "Zongyu"
			},
			{
				"family": "Feng",
				"given": "Siyuan"
			},
			{
				"family": "Tu",
				"given": "Ming"
			},
			{
				"family": "Ji",
				"given": "Yuliang"
			},
			{
				"family": "Xia",
				"given": "Rui"
			},
			{
				"family": "Ma",
				"given": "Mingbo"
			},
			{
				"family": "Song",
				"given": "Xuchen"
			},
			{
				"family": "Chen",
				"given": "Jitong"
			},
			{
				"family": "Wang",
				"given": "Yuping"
			},
			{
				"family": "Wang",
				"given": "Yuxuan"
			}
		]
	},
	{
		"id": "http://zotero.org/users/14181977/items/SEEZ3SEN",
		"type": "webpage",
		"title": "Preface — Digital Signals Theory",
		"URL": "https://brianmcfee.net/dstbook-site/content/intro.html",
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/PP9EPTZ3",
		"type": "software",
		"abstract": "Think DSP: Digital Signal Processing in Python, by Allen B. Downey.",
		"genre": "Jupyter Notebook",
		"note": "original-date: 2013-08-13T18:37:02Z",
		"source": "GitHub",
		"title": "AllenDowney/ThinkDSP",
		"URL": "https://github.com/AllenDowney/ThinkDSP",
		"author": [
			{
				"family": "Downey",
				"given": "Allen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					27
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					4,
					26
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/14181977/items/XZLVMFU4",
		"type": "book"
	},
	{
		"id": "http://zotero.org/users/14181977/items/EI7RMTDK",
		"type": "book"
	},
	{
		"id": "http://zotero.org/users/14181977/items/IY2L9N58",
		"type": "document",
		"title": "Generative Deep Learning - David Foster.epub"
	}
]