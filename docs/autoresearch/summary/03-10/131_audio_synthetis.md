# Research Papers Summary

## Generative Audio Synthesis with a Parametric Model

- **ID**: http://arxiv.org/abs/1911.08335v1
- **Published**: 2019-11-15T20:59:30Z
- **Authors**: Krishna Subramani, Alexandre D'Hooge, Preeti Rao
- **Categories**: , , 

### GPT Summary
This paper explores the use of a parametric representation of audio to enhance the training of generative models, allowing for improved flexibility in sound generation.

### New Contributions
The research introduces a novel approach to audio representation that enables more nuanced control over the characteristics of generated sounds, potentially leading to advancements in generative audio applications.

### Tags
parametric audio representation, generative sound models, audio synthesis control, flexible sound generation, musical generative models, sound manipulation techniques, audio feature extraction, machine listening, computational musicology

### PDF Link
[Link](http://arxiv.org/abs/1911.08335v1)

---

## Continuous descriptor-based control for deep audio synthesis

- **ID**: http://arxiv.org/abs/2302.13542v1
- **Published**: 2023-02-27T06:40:11Z
- **Authors**: Ninon Devis, Nils Demerlé, Sarah Nabi, David Genova, Philippe Esling
- **Categories**: , , 

### GPT Summary
This paper presents a lightweight deep generative audio model that enables expressive, continuous control over sound generation through descriptor-based conditioning, making it accessible for musicians and creative workflows.

### New Contributions
The authors introduce an adversarial confusion criterion to enhance controllability in latent space by removing salient musical features, allowing users to specify and reintroduce features for real-time sound generation akin to traditional synthesizer controls.

### Tags
expressive control, deep generative audio, descriptor-based conditioning, latent space manipulation, real-time sound generation, musical feature removal, timbre transfer, synthesizer integration, lightweight models

### PDF Link
[Link](http://arxiv.org/abs/2302.13542v1)

---

## Deep generative models for musical audio synthesis

- **ID**: http://arxiv.org/abs/2006.06426v2
- **Published**: 2020-06-10T04:02:42Z
- **Authors**: M. Huzaifah, L. Wyse
- **Categories**: , , , , 

### GPT Summary
This paper reviews recent advancements in deep learning that are transforming sound modeling practices, particularly in the context of generative audio synthesis. It highlights the shift towards using machine learning systems to facilitate model training and control strategies for sound generation.

### New Contributions
The paper introduces a comprehensive overview of how generative deep learning techniques are revolutionizing sound modeling by enabling more efficient synthesis and control compared to traditional methods, addressing the labor-intensive nature of previous approaches.

### Tags
generative audio synthesis, deep learning for sound, sound modeling techniques, control strategies in audio, audio generation algorithms, machine learning in sound design, parametric sound control, acoustic feature manipulation, audio synthesis advancements

### PDF Link
[Link](http://arxiv.org/abs/2006.06426v2)

---

## Full-band General Audio Synthesis with Score-based Diffusion

- **ID**: http://arxiv.org/abs/2210.14661v1
- **Published**: 2022-10-26T12:25:57Z
- **Authors**: Santiago Pascual, Gautam Bhattacharya, Chunghsin Yeh, Jordi Pons, Joan Serrà
- **Categories**: , , 

### GPT Summary
This paper introduces DAG, a diffusion-based generative model for audio synthesis that operates on full-band signals in the waveform domain, outperforming existing label-conditioned generators in quality and diversity.

### New Contributions
The study presents a novel approach to audio synthesis using a diffusion-based model that achieves significant improvements of up to 65% in quality over state-of-the-art methods, while also demonstrating flexibility in accommodating various conditioning schemas.

### Tags
diffusion models, audio synthesis, full-band signals, label conditioning, generative models, waveform domain, signal processing, model evaluation, quality improvement

### PDF Link
[Link](http://arxiv.org/abs/2210.14661v1)

---

## Adversarial Audio Synthesis

- **ID**: http://arxiv.org/abs/1802.04208v3
- **Published**: 2018-02-12T17:50:43Z
- **Authors**: Chris Donahue, Julian McAuley, Miller Puckette
- **Categories**: , 

### GPT Summary
This paper presents WaveGAN, a novel approach that applies generative adversarial networks (GANs) to the unsupervised synthesis of raw-waveform audio, demonstrating its ability to generate coherent audio samples across various domains without the need for labeled data.

### New Contributions
WaveGAN introduces a pioneering method for audio generation using GANs, achieving global coherence in synthesized audio waveforms and successfully producing intelligible speech and sounds from diverse categories like drums and bird vocalizations.

### Tags
WaveGAN, audio synthesis, generative adversarial networks, raw-waveform audio, unsupervised learning, sound effect generation, intelligible speech synthesis, multi-domain audio generation, coherent audio samples

### PDF Link
[Link](http://arxiv.org/abs/1802.04208v3)

---

## GANSynth: Adversarial Neural Audio Synthesis

- **ID**: http://arxiv.org/abs/1902.08710v2
- **Published**: 2019-02-23T00:55:16Z
- **Authors**: Jesse Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue, Adam Roberts
- **Categories**: , , , 

### GPT Summary
This paper presents a novel approach to using Generative Adversarial Networks (GANs) for high-fidelity audio synthesis, demonstrating their ability to generate locally-coherent audio efficiently by modeling log magnitudes and instantaneous frequencies in the spectral domain.

### New Contributions
The study reveals that GANs can outperform WaveNet models in both automated and human evaluation metrics for audio quality, while also achieving significantly faster audio generation, by utilizing an improved modeling technique that enhances frequency resolution.

### Tags
Generative Adversarial Networks, audio synthesis, waveform coherence, NSynth dataset, spectral domain modeling, log magnitudes, instantaneous frequencies, high-fidelity audio, efficient audio generation

### PDF Link
[Link](http://arxiv.org/abs/1902.08710v2)

---

## Efficient Neural Audio Synthesis

- **ID**: http://arxiv.org/abs/1802.08435v2
- **Published**: 2018-02-23T08:20:23Z
- **Authors**: Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, Koray Kavukcuoglu
- **Categories**: , , 

### GPT Summary
This paper presents techniques to enhance sampling efficiency in text-to-speech synthesis using a novel WaveRNN architecture, demonstrating significantly faster audio generation without compromising quality.

### New Contributions
The authors introduce a WaveRNN with a dual softmax layer that matches the performance of WaveNet while being four times faster in generation. They also explore weight pruning for improved performance in sparse networks and propose a new generation scheme, Subscale WaveRNN, that allows for simultaneous sample generation without quality loss.

### Tags
text-to-speech synthesis, WaveRNN, sampling efficiency, weight pruning, sparse neural networks, audio generation, Subscale WaveRNN, real-time audio, high-fidelity audio

### PDF Link
[Link](http://arxiv.org/abs/1802.08435v2)

---

## A Survey on Audio Synthesis and Audio-Visual Multimodal Processing

- **ID**: http://arxiv.org/abs/2108.00443v1
- **Published**: 2021-08-01T12:35:16Z
- **Authors**: Zhaofeng Shi
- **Categories**: , 

### GPT Summary
This paper presents a comprehensive survey on audio synthesis and audio-visual multimodal processing, focusing on text-to-speech, music generation, and the integration of visual and acoustic information. The review categorizes current technical methods and discusses future trends in these fields.

### New Contributions
The paper classifies and introduces various technical methods in audio synthesis and audio-visual multimodal processing while prospecting future development trends, providing valuable insights for researchers in these areas.

### Tags
audio synthesis, multimodal processing, text-to-speech, music generation, audio-visual integration, future trends, technical classification, research survey, deep learning in audio

### PDF Link
[Link](http://arxiv.org/abs/2108.00443v1)

---

## Towards Lightweight Controllable Audio Synthesis with Conditional  Implicit Neural Representations

- **ID**: http://arxiv.org/abs/2111.08462v2
- **Published**: 2021-11-14T13:36:18Z
- **Authors**: Jan Zuiderveld, Marco Federici, Erik J. Bekkers
- **Categories**: , 

### GPT Summary
This paper investigates the use of Conditional Implicit Neural Representations (CINRs) for audio synthesis, demonstrating that small Periodic Conditional INRs (PCINRs) outperform traditional Transposed Convolutional Neural Networks in terms of learning speed and audio reconstruction quality, though they exhibit sensitivity to hyperparameters and can introduce noise in certain contexts.

### New Contributions
The study introduces PCINRs as effective lightweight models for audio synthesis, providing insights into their performance advantages over conventional neural architectures and identifying strategies for mitigating noise in audio reconstructions.

### Tags
Conditional Implicit Neural Representations, audio synthesis, Periodic Conditional INRs, neural audio reconstruction, hyperparameter sensitivity, weight regularization, generative audio frameworks, high-frequency noise, real-time audio processing

### PDF Link
[Link](http://arxiv.org/abs/2111.08462v2)

---

## Generative Modelling for Controllable Audio Synthesis of Expressive  Piano Performance

- **ID**: http://arxiv.org/abs/2006.09833v2
- **Published**: 2020-06-16T12:54:41Z
- **Authors**: Hao Hao Tan, Yin-Jyun Luo, Dorien Herremans
- **Categories**: , , , 

### GPT Summary
This paper introduces a controllable neural audio synthesizer using Gaussian Mixture Variational Autoencoders (GM-VAE) that generates realistic piano performances by adhering to specific temporal conditions related to articulation and dynamics.

### New Contributions
The model enables fine-grained style morphing during audio synthesis by utilizing latent variables to condition the synthesis process, thereby offering new creative interpretations of existing piano music.

### Tags
neural audio synthesis, Gaussian Mixture Variational Autoencoders, piano performance, articulation control, dynamics modeling, style morphing, latent variable conditioning, musical interpretation, generative models

### PDF Link
[Link](http://arxiv.org/abs/2006.09833v2)

---

## An Audio Synthesis Framework Derived from Industrial Process Control

- **ID**: http://arxiv.org/abs/2109.10455v1
- **Published**: 2021-09-21T23:20:51Z
- **Authors**: Ashwin Pillay
- **Categories**: , 

### GPT Summary
This research introduces a novel audio synthesis technique by redefining the Proportional-Integral-Derivative (PID) algorithm and implementing it in a Python application to analyze its control parameters and synthesized outputs. The study explores its applications as an audio signal and LFO generator, suggesting its potential as an alternative to existing synthesis methods like FM and Wavetable synthesis.

### New Contributions
The paper presents a new framework for sound synthesis based on a modified PID algorithm, providing insights into its control parameters and applications in audio generation, which may enhance current synthesis techniques and inspire further research.

### Tags
audio synthesis, Proportional-Integral-Derivative, sound generation, LFO generator, FM synthesis alternative, Wavetable synthesis, synthesis techniques, control parameters, digital audio processing

### PDF Link
[Link](http://arxiv.org/abs/2109.10455v1)

---

## Upsampling artifacts in neural audio synthesis

- **ID**: http://arxiv.org/abs/2010.14356v2
- **Published**: 2020-10-27T15:09:28Z
- **Authors**: Jordi Pons, Santiago Pascual, Giulio Cengarle, Joan Serrà
- **Categories**: , , 

### GPT Summary
This paper investigates upsampling artifacts in neural audio synthesis, highlighting their sources and effects, which have been largely neglected in audio processing despite being recognized in computer vision. The authors compare various upsampling methods, advocating for nearest neighbor upsampling as a viable alternative to more commonly used techniques that introduce tonal artifacts.

### New Contributions
The paper identifies and analyzes the specific sources of upsampling artifacts in audio processing and provides a comparative assessment of different upsampling layers, recommending nearest neighbor upsampling as a less artifact-prone alternative.

### Tags
upsampling artifacts, neural audio synthesis, audio signal processing, nearest neighbor upsampling, tonal artifacts, spectral replicas, transposed convolutions, subpixel convolutions, audio quality improvement

### PDF Link
[Link](http://arxiv.org/abs/2010.14356v2)

---

## DarkGAN: Exploiting Knowledge Distillation for Comprehensible Audio  Synthesis with GANs

- **ID**: http://arxiv.org/abs/2108.01216v1
- **Published**: 2021-08-03T00:26:55Z
- **Authors**: Javier Nistal, Stefan Lattner, Gaël Richard
- **Categories**: , 

### GPT Summary
This paper presents DarkGAN, an adversarial audio synthesizer that leverages knowledge distillation from an automatic audio-tagging system to enhance control over audio synthesis despite the lack of semantic annotations in musical datasets. The results indicate that DarkGAN can produce high-quality musical audio with moderate control over attributes using generated soft labels.

### New Contributions
The paper introduces a novel approach to audio synthesis by utilizing soft labels generated from an automatic audio-tagging system for knowledge distillation into a GAN framework, thereby addressing the challenge of controlling GANs with meaningful metadata in the absence of traditional annotations.

### Tags
Generative Adversarial Networks, audio synthesis, knowledge distillation, automatic audio tagging, soft labels, musical audio generation, attribute control, semantic conditioning, DarkGAN

### PDF Link
[Link](http://arxiv.org/abs/2108.01216v1)

---

## SnakeSynth: New Interactions for Generative Audio Synthesis

- **ID**: http://arxiv.org/abs/2307.05830v1
- **Published**: 2023-07-11T22:51:54Z
- **Authors**: Eric Easthope
- **Categories**: , , 

### GPT Summary
The paper introduces SnakeSynth, a web-based audio synthesizer that utilizes a deep generative model to create variable-length generative sounds controlled by real-time 2D interaction gestures. It demonstrates how touch and mobile-compatible gestures can modulate sound length and intensity through a programmable 2D grid.

### New Contributions
SnakeSynth offers a novel approach to musical expression by combining deep generative sound synthesis with real-time, intuitive 2D interaction, allowing users to manipulate audio playback dynamically without extensive training times for the model.

### Tags
generative audio synthesis, real-time interaction, 2D gesture control, deep generative models, web-based audio, musical expression, interactive sound design, TensorFlow.js, audio modulation, programmable sound control

### PDF Link
[Link](http://arxiv.org/abs/2307.05830v1)

---

## Streamable Neural Audio Synthesis With Non-Causal Convolutions

- **ID**: http://arxiv.org/abs/2204.07064v1
- **Published**: 2022-04-14T16:00:32Z
- **Authors**: Antoine Caillon, Philippe Esling
- **Categories**: , , , 

### GPT Summary
This paper presents a novel method for transforming convolutional models into non-causal streaming models, enabling real-time buffer-based audio processing without compromising audio quality. The approach is demonstrated on the RAVE model, showcasing its efficiency and applicability across various music and speech datasets.

### New Contributions
The paper introduces a method for post-training reconfiguration of convolutional models to make them compatible with real-time audio generation, effectively converting models trained without causal constraints into streaming models, and providing implementations for use in digital audio workstations.

### Tags
real-time audio synthesis, non-causal streaming models, convolutional networks, post-training reconfiguration, RAVE model, digital audio workstations, Max/MSP, PureData externals, VST audio plugin

### PDF Link
[Link](http://arxiv.org/abs/2204.07064v1)

---

## Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders

- **ID**: http://arxiv.org/abs/1704.01279v1
- **Published**: 2017-04-05T06:34:22Z
- **Authors**: Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, Mohammad Norouzi
- **Categories**: , , 

### GPT Summary
This paper presents a novel WaveNet-style autoencoder for audio modeling, leveraging a large-scale dataset of musical notes called NSynth to enhance the performance of generative audio models. The authors demonstrate that their model can interpolate between different instruments, creating realistic and expressive new sounds.

### New Contributions
The paper introduces a powerful WaveNet-style autoencoder that conditions on temporal codes from raw audio, and a significantly larger dataset, NSynth, which enables improved performance in audio generation and allows for meaningful timbre interpolation between instruments.

### Tags
WaveNet, audio modeling, musical dataset, NSynth, autoencoder, timbre interpolation, sound morphing, generative audio, temporal coding

### PDF Link
[Link](http://arxiv.org/abs/1704.01279v1)

---

## Text-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis

- **ID**: http://arxiv.org/abs/2104.12292v6
- **Published**: 2021-04-25T23:59:00Z
- **Authors**: Erica Cooper, Xin Wang, Junichi Yamagishi
- **Categories**: , 

### GPT Summary
This study explores the application of text-to-speech synthesis techniques, particularly Tacotron and neural source-filter models, in the context of piano MIDI-to-audio synthesis, revealing both potential and challenges in this novel approach. Despite the promising integration of TTS components, the MIDI-to-audio synthesis system still lags behind traditional sound modeling methods.

### New Contributions
The paper introduces a novel application of TTS synthesis techniques to piano MIDI-to-audio synthesis, demonstrating that with minor modifications, these methods can be adapted for musical audio generation while highlighting the difficulties in converting MIDI to acoustic features.

### Tags
MIDI-to-audio synthesis, text-to-speech synthesis, Tacotron, neural source-filter models, piano sound synthesis, audio generation techniques, sound modeling, acoustic feature extraction, music technology, novel applications in synthesis

### PDF Link
[Link](http://arxiv.org/abs/2104.12292v6)

---

## Comparing Representations for Audio Synthesis Using Generative  Adversarial Networks

- **ID**: http://arxiv.org/abs/2006.09266v2
- **Published**: 2020-06-16T15:48:17Z
- **Authors**: Javier Nistal, Stefan Lattner, Gaël Richard
- **Categories**: , 

### GPT Summary
This paper investigates various audio signal representations for audio synthesis using Generative Adversarial Networks (GANs), specifically comparing raw audio and time-frequency representations on the NSynth dataset. The study finds that complex-valued representations and specific features of the Short-Time Fourier Transform outperform others in generating high-quality audio efficiently.

### New Contributions
The paper introduces a comparative analysis of different audio signal representations for GANs in audio synthesis, demonstrating that complex-valued and certain time-frequency features yield superior performance in terms of audio quality and generation speed.

### Tags
audio synthesis, Generative Adversarial Networks, time-frequency representation, NSynth dataset, Short-Time Fourier Transform, complex-valued representations, pitch conditioning, performance evaluation, generative models

### PDF Link
[Link](http://arxiv.org/abs/2006.09266v2)

---

## VaPar Synth -- A Variational Parametric Model for Audio Synthesis

- **ID**: http://arxiv.org/abs/2004.00001v1
- **Published**: 2020-03-30T16:05:47Z
- **Authors**: Krishna Subramani, Preeti Rao, Alexandre D'Hooge
- **Categories**: , , 

### GPT Summary
The paper introduces VaPar Synth, a Variational Parametric Synthesizer that employs a conditional variational autoencoder for audio synthesis, allowing for flexible control over musical attributes such as pitch, dynamics, and timbre.

### New Contributions
The key contribution is the development of VaPar Synth, which leverages a parametric representation of audio signals to enhance control over generated sound, demonstrating effective reconstruction and generation of instrumental tones.

### Tags
Variational Parametric Synthesizer, Conditional Variational Autoencoder, Audio Synthesis, Parametric Representation, Instrumental Tone Generation, Musical Attributes Control, Pitch Manipulation, Deep Learning in Audio, Statistical Modeling in Music

### PDF Link
[Link](http://dx.doi.org/10.1109/ICASSP40776.2020.9054181)

---

## Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion  Models

- **ID**: http://arxiv.org/abs/2306.17203v1
- **Published**: 2023-06-29T12:39:58Z
- **Authors**: Simian Luo, Chuanhao Yan, Chenxu Hu, Hang Zhao
- **Categories**: , , , 

### GPT Summary
The paper introduces Diff-Foley, a novel Video-to-Audio synthesis method that utilizes a latent diffusion model to generate high-quality audio from silent videos with enhanced synchronization and audio-visual relevance. By employing contrastive audio-visual pretraining and a cross-attention module, Diff-Foley achieves state-of-the-art performance in V2A tasks.

### New Contributions
Diff-Foley's key contributions include the use of contrastive audio-visual pretraining to align audio and visual features temporally and semantically, the implementation of a cross-attention module for capturing audio-visual correlations, and the introduction of 'double guidance' to enhance sample quality, leading to improved performance on large-scale V2A datasets.

### Tags
Video-to-Audio synthesis, latent diffusion models, audio-visual synchronization, contrastive pretraining, cross-attention mechanisms, spectrogram generation, double guidance, temporal alignment, audio-visual relevance, state-of-the-art V2A performance

### PDF Link
[Link](http://arxiv.org/abs/2306.17203v1)

---

